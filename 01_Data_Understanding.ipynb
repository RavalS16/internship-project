{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb7a8b8",
   "metadata": {},
   "source": [
    "# SPRINT 1 — Data Understanding & Cleaning\n",
    "\n",
    "## Task 1: Load the dataset & print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset from Excel file\n",
    "file_path = '../data/Cleaned_Viral_Social_Media_Trends.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET SCHEMA INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print shape\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"  - Rows: {df.shape[0]}\")\n",
    "print(f\"  - Columns: {df.shape[1]}\")\n",
    "\n",
    "# Print info()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()\n",
    "\n",
    "# Print head()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"First 5 Rows:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646946d9",
   "metadata": {},
   "source": [
    "## Task 2: Detect and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Detect and remove duplicates\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 2: DETECT AND REMOVE DUPLICATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for duplicates before removal\n",
    "duplicates_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows (before removal): {duplicates_count}\")\n",
    "\n",
    "# Get duplicate rows by all columns\n",
    "print(f\"\\nDuplicate rows by all columns: {df.duplicated().sum()}\")\n",
    "\n",
    "# Get duplicate rows by specific columns (if needed to check specific column duplicates)\n",
    "print(f\"\\nDataset shape before removing duplicates: {df.shape}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "print(f\"Dataset shape after removing duplicates: {df_cleaned.shape}\")\n",
    "\n",
    "# Count of removed duplicates\n",
    "removed_count = df.shape[0] - df_cleaned.shape[0]\n",
    "print(f\"\\nTotal duplicates removed: {removed_count}\")\n",
    "\n",
    "# Update df to the cleaned version\n",
    "df = df_cleaned\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Duplicates removed successfully. Dataset now has {df.shape[0]} unique rows.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Handle missing values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 3: HANDLE MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate missing value report\n",
    "print(\"\\nMissing Value Report:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_report = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(missing_report.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_percentage = (total_missing / total_cells * 100).round(2)\n",
    "\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n",
    "print(f\"Total cells: {total_cells}\")\n",
    "print(f\"Overall missing percentage: {missing_percentage}%\")\n",
    "\n",
    "# Handle missing values based on data type\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Handling missing values:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_filled = df.copy()\n",
    "\n",
    "# For numeric columns: fill with median\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "for col in numeric_cols:\n",
    "    if df_filled[col].isnull().sum() > 0:\n",
    "        median_val = df_filled[col].median()\n",
    "        df_filled[col].fillna(median_val, inplace=True)\n",
    "        print(f\"  ✓ {col}: filled {df[col].isnull().sum()} missing values with median ({median_val:.2f})\")\n",
    "\n",
    "# For categorical columns: fill with mode\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_filled[col].isnull().sum() > 0:\n",
    "        mode_val = df_filled[col].mode()[0]\n",
    "        df_filled[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"  ✓ {col}: filled {df[col].isnull().sum()} missing values with mode ('{mode_val}')\")\n",
    "\n",
    "# Verify missing values after handling\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verification after handling missing values:\")\n",
    "print(\"-\" * 80)\n",
    "remaining_missing = df_filled.isnull().sum().sum()\n",
    "print(f\"Total remaining missing values: {remaining_missing}\")\n",
    "\n",
    "if remaining_missing == 0:\n",
    "    print(\"✓ All missing values have been handled successfully!\")\n",
    "else:\n",
    "    print(f\"⚠ Still {remaining_missing} missing values remaining\")\n",
    "\n",
    "# Update df to the cleaned version\n",
    "df = df_filled\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Missing values handled. Dataset now has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d2759",
   "metadata": {},
   "source": [
    "## Task 3: Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20271e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 (single cleaned_text column)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 4 (single column): TEXT NORMALIZATION -> cleaned_text\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text according to requirements:\n",
    "    - lowercase\n",
    "    - remove URLs\n",
    "    - remove punctuations\n",
    "    - remove numbers\n",
    "    - remove extra spaces\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|https\\S+', '', text)\n",
    "    # remove punctuation (keep alphanumeric and spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # replace underscores (from \\w) and collapse spaces\n",
    "    text = text.replace('_', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Choose a primary raw text column to create `cleaned_text` from\n",
    "candidates = [col for col in df.columns \n",
    "              if df[col].dtype == 'object' \n",
    "              and '_normalized' not in col \n",
    "              and '_no_stopwords' not in col \n",
    "              and '_hashtags' not in col]\n",
    "\n",
    "if candidates:\n",
    "    primary_text_col = candidates[0]\n",
    "    print(f\"Using primary text column: {primary_text_col} -> creating 'cleaned_text'.\")\n",
    "    df['cleaned_text'] = df[primary_text_col].apply(normalize_text)\n",
    "    print(f\"\\nDataset shape after adding 'cleaned_text': {df.shape}\")\n",
    "    print(\"\\nSample cleaned_text (first 5 rows):\")\n",
    "    print(df['cleaned_text'].head().to_string(index=False))\n",
    "else:\n",
    "    print(\"No suitable text column found to create 'cleaned_text'.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c93ca",
   "metadata": {},
   "source": [
    "## Task 5: Remove stopwords\n",
    "\n",
    "Input: `cleaned_text`\n",
    "Output: `text_no_stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Remove stopwords\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 5: REMOVE STOPWORDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Import nltk stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already present\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"\\nLoaded {len(stop_words)} English stopwords\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")\n",
    "\n",
    "# Define function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply to cleaned_text column\n",
    "if 'cleaned_text' in df.columns:\n",
    "    print(\"\\nApplying stopword removal to 'cleaned_text'...\")\n",
    "    df['text_no_stopwords'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "    print(f\"✓ Created 'text_no_stopwords' column\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Show comparison\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Sample comparison (cleaned_text vs text_no_stopwords):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sample_df = df[['cleaned_text', 'text_no_stopwords']].head(5)\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nRow {idx}:\")\n",
    "        print(f\"  cleaned_text: {row['cleaned_text'][:80]}...\")\n",
    "        print(f\"  text_no_stopwords: {row['text_no_stopwords'][:80]}...\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    empty_count = (df['text_no_stopwords'] == '').sum()\n",
    "    non_empty_count = len(df) - empty_count\n",
    "    print(f\"Non-empty text_no_stopwords: {non_empty_count} / {len(df)}\")\n",
    "    print(f\"Empty text_no_stopwords: {empty_count} / {len(df)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ 'cleaned_text' column not found. Please run Task 4 first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 5 completed: Stopwords removed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01128128",
   "metadata": {},
   "source": [
    "## Task 6: Extract hashtags\n",
    "\n",
    "Input: raw text\n",
    "Output: `hashtags` column (list of hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Extract hashtags\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 6: EXTRACT HASHTAGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define function to extract hashtags\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract all hashtags from text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = str(text)\n",
    "    # Match hashtag pattern: # followed by word characters\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    return hashtags\n",
    "\n",
    "# Find the primary text column (raw text, not processed)\n",
    "candidates = [col for col in df.columns \n",
    "              if df[col].dtype == 'object' \n",
    "              and '_normalized' not in col \n",
    "              and '_no_stopwords' not in col \n",
    "              and 'cleaned' not in col]\n",
    "\n",
    "if candidates:\n",
    "    raw_text_col = candidates[0]\n",
    "    print(f\"Extracting hashtags from column: '{raw_text_col}'\")\n",
    "    df['hashtags'] = df[raw_text_col].apply(extract_hashtags)\n",
    "    print(f\"✓ Created 'hashtags' column\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Hashtag Extraction Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    rows_with_hashtags = (df['hashtags'].apply(len) > 0).sum()\n",
    "    rows_without_hashtags = total_rows - rows_with_hashtags\n",
    "    total_hashtags = sum(len(h) for h in df['hashtags'])\n",
    "    avg_hashtags_per_row = total_hashtags / total_rows if total_rows > 0 else 0\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows with hashtags: {rows_with_hashtags} ({rows_with_hashtags/total_rows*100:.2f}%)\")\n",
    "    print(f\"Rows without hashtags: {rows_without_hashtags} ({rows_without_hashtags/total_rows*100:.2f}%)\")\n",
    "    print(f\"Total hashtags extracted: {total_hashtags}\")\n",
    "    print(f\"Average hashtags per row: {avg_hashtags_per_row:.2f}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Sample hashtags (first 10 rows with hashtags):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    rows_with_tags = df[df['hashtags'].apply(len) > 0].head(10)\n",
    "    for idx, (i, row) in enumerate(rows_with_tags.iterrows(), 1):\n",
    "        hashtag_list = ', '.join(row['hashtags'][:5])  # Show max 5 hashtags\n",
    "        if len(row['hashtags']) > 5:\n",
    "            hashtag_list += f\", ... (+{len(row['hashtags']) - 5} more)\"\n",
    "        print(f\"{idx}. {hashtag_list}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No suitable raw text column found to extract hashtags.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 6 completed: Hashtags extracted successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bddd21",
   "metadata": {},
   "source": [
    "## Task 7: Calculate sentiment scores\n",
    "\n",
    "Input: clean text\n",
    "Output: `polarity`, `subjectivity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32970ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Calculate sentiment scores\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 7: CALCULATE SENTIMENT SCORES (POLARITY & SUBJECTIVITY)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try to import TextBlob, install if missing\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    print(\"TextBlob not found. Installing textblob...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'textblob'])\n",
    "    from textblob import TextBlob\n",
    "    # Ensure punkt tokenizer available\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except Exception:\n",
    "        import nltk\n",
    "        print('Downloading NLTK punkt tokenizer...')\n",
    "        nltk.download('punkt')\n",
    "\n",
    "# Choose input column: prefer 'text_no_stopwords', then 'cleaned_text', then common raw text names\n",
    "candidates = ['text_no_stopwords', 'cleaned_text', 'text', 'tweet', 'content', 'post', 'message', 'caption', 'body']\n",
    "input_col = None\n",
    "for c in candidates:\n",
    "    if c in df.columns:\n",
    "        input_col = c\n",
    "        break\n",
    "\n",
    "if input_col is None:\n",
    "    raise ValueError('No suitable text column found for sentiment calculation. Please create `cleaned_text` or `text_no_stopwords` first.')\n",
    "\n",
    "print(f\"Using '{input_col}' as input for sentiment calculation\")\n",
    "\n",
    "# Function to compute polarity & subjectivity\n",
    "def compute_sentiment(text):\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return (None, None)\n",
    "    try:\n",
    "        s = TextBlob(str(text))\n",
    "        return (s.sentiment.polarity, s.sentiment.subjectivity)\n",
    "    except Exception as e:\n",
    "        return (None, None)\n",
    "\n",
    "# Apply to the dataframe\n",
    "sentiments = df[input_col].fillna('').apply(lambda t: compute_sentiment(t))\n",
    "\n",
    "df['polarity'] = sentiments.apply(lambda x: x[0])\n",
    "df['subjectivity'] = sentiments.apply(lambda x: x[1])\n",
    "\n",
    "# Quick stats and sample\n",
    "print(f\"\\nAdded columns: 'polarity' and 'subjectivity' to DataFrame. Dataset shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nSample sentiment scores (first 10 rows):\")\n",
    "print(df[[input_col, 'polarity', 'subjectivity']].head(10).to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "valid_polarity = df['polarity'].dropna()\n",
    "valid_subjectivity = df['subjectivity'].dropna()\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Sentiment summary:\")\n",
    "print(\"-\" * 80)\n",
    "if len(valid_polarity) > 0:\n",
    "    print(f\"Polarity: mean={valid_polarity.mean():.4f}, min={valid_polarity.min():.4f}, max={valid_polarity.max():.4f}\")\n",
    "else:\n",
    "    print(\"No valid polarity values computed.\")\n",
    "\n",
    "if len(valid_subjectivity) > 0:\n",
    "    print(f\"Subjectivity: mean={valid_subjectivity.mean():.4f}, min={valid_subjectivity.min():.4f}, max={valid_subjectivity.max():.4f}\")\n",
    "else:\n",
    "    print(\"No valid subjectivity values computed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 7 completed: Sentiment scores calculated.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab805dab",
   "metadata": {},
   "source": [
    "## Task 8: Export cleaned dataset\n",
    "\n",
    "Input: processed DataFrame\n",
    "Output: CSV → `processed/clean_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74787df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Export cleaned dataset\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 8: EXPORT CLEANED DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define output file path\n",
    "output_file = os.path.join(output_dir, 'clean_data.csv')\n",
    "\n",
    "# Export to CSV\n",
    "print(f\"\\nExporting cleaned DataFrame to: {output_file}\")\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "# Verify export\n",
    "if os.path.exists(output_file):\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"✓ Export successful!\")\n",
    "    print(f\"  - File path: {output_file}\")\n",
    "    print(f\"  - File size: {file_size:.2f} MB\")\n",
    "    print(f\"  - Rows: {len(df)}\")\n",
    "    print(f\"  - Columns: {len(df.columns)}\")\n",
    "    print(f\"\\n  Columns in cleaned dataset:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"    {i}. {col}\")\n",
    "else:\n",
    "    print(f\"⚠ Export failed. File not found at {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 8 completed: Cleaned dataset exported successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4399a",
   "metadata": {},
   "source": [
    "# SPRINT 2 — Sentiment Drivers + Hashtag Intelligence\n",
    "\n",
    "## Task 9: Generate unigrams/bigrams/trigrams\n",
    "\n",
    "Output: frequency tables for positive & negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee954d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9: Generate unigrams/bigrams/trigrams\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 9: GENERATE N-GRAMS (UNIGRAMS, BIGRAMS, TRIGRAMS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Define sentiment thresholds\n",
    "# Positive: polarity > 0.1, Negative: polarity < -0.1, Neutral: in between\n",
    "positive_threshold = 0.1\n",
    "negative_threshold = -0.1\n",
    "\n",
    "# Categorize rows by sentiment\n",
    "df['sentiment_category'] = df['polarity'].apply(\n",
    "    lambda x: 'positive' if x > positive_threshold \n",
    "              else ('negative' if x < negative_threshold else 'neutral')\n",
    ")\n",
    "\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(f\"  Positive (polarity > {positive_threshold}): {(df['sentiment_category'] == 'positive').sum()}\")\n",
    "print(f\"  Negative (polarity < {negative_threshold}): {(df['sentiment_category'] == 'negative').sum()}\")\n",
    "print(f\"  Neutral: {(df['sentiment_category'] == 'neutral').sum()}\")\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"Generate n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return []\n",
    "    words = str(text).split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Generate n-grams for positive and negative sentiments\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Generating N-grams for Positive Sentiment:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "positive_texts = df[df['sentiment_category'] == 'positive']['text_no_stopwords']\n",
    "positive_unigrams = Counter()\n",
    "positive_bigrams = Counter()\n",
    "positive_trigrams = Counter()\n",
    "\n",
    "for text in positive_texts:\n",
    "    positive_unigrams.update(generate_ngrams(text, 1))\n",
    "    positive_bigrams.update(generate_ngrams(text, 2))\n",
    "    positive_trigrams.update(generate_ngrams(text, 3))\n",
    "\n",
    "print(f\"Top 10 Unigrams (Positive):\")\n",
    "for gram, freq in positive_unigrams.most_common(10):\n",
    "    print(f\"  {gram[0]}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Bigrams (Positive):\")\n",
    "for gram, freq in positive_bigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Trigrams (Positive):\")\n",
    "for gram, freq in positive_trigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "# Generate n-grams for negative sentiment\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Generating N-grams for Negative Sentiment:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "negative_texts = df[df['sentiment_category'] == 'negative']['text_no_stopwords']\n",
    "negative_unigrams = Counter()\n",
    "negative_bigrams = Counter()\n",
    "negative_trigrams = Counter()\n",
    "\n",
    "for text in negative_texts:\n",
    "    negative_unigrams.update(generate_ngrams(text, 1))\n",
    "    negative_bigrams.update(generate_ngrams(text, 2))\n",
    "    negative_trigrams.update(generate_ngrams(text, 3))\n",
    "\n",
    "print(f\"Top 10 Unigrams (Negative):\")\n",
    "for gram, freq in negative_unigrams.most_common(10):\n",
    "    print(f\"  {gram[0]}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Bigrams (Negative):\")\n",
    "for gram, freq in negative_bigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Trigrams (Negative):\")\n",
    "for gram, freq in negative_trigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "# Create frequency DataFrames for export\n",
    "pos_unigram_df = pd.DataFrame(\n",
    "    [(gram[0], freq) for gram, freq in positive_unigrams.most_common(50)],\n",
    "    columns=['unigram', 'frequency']\n",
    ")\n",
    "pos_bigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in positive_bigrams.most_common(50)],\n",
    "    columns=['bigram', 'frequency']\n",
    ")\n",
    "pos_trigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in positive_trigrams.most_common(50)],\n",
    "    columns=['trigram', 'frequency']\n",
    ")\n",
    "\n",
    "neg_unigram_df = pd.DataFrame(\n",
    "    [(gram[0], freq) for gram, freq in negative_unigrams.most_common(50)],\n",
    "    columns=['unigram', 'frequency']\n",
    ")\n",
    "neg_bigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in negative_bigrams.most_common(50)],\n",
    "    columns=['bigram', 'frequency']\n",
    ")\n",
    "neg_trigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in negative_trigrams.most_common(50)],\n",
    "    columns=['trigram', 'frequency']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Positive unigrams: {len(positive_unigrams)}\")\n",
    "print(f\"Positive bigrams: {len(positive_bigrams)}\")\n",
    "print(f\"Positive trigrams: {len(positive_trigrams)}\")\n",
    "print(f\"Negative unigrams: {len(negative_unigrams)}\")\n",
    "print(f\"Negative bigrams: {len(negative_bigrams)}\")\n",
    "print(f\"Negative trigrams: {len(negative_trigrams)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 9 completed: N-grams generated and analyzed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ccca0",
   "metadata": {},
   "source": [
    "## Task 10: Create TF-IDF vectors (positive vs negative groups)\n",
    "\n",
    "Output: top TF-IDF keywords per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8644658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10: Create TF-IDF vectors (positive vs negative groups)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 10: CREATE TF-IDF VECTORS (POSITIVE VS NEGATIVE GROUPS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Select sentiment column\n",
    "sentiment_col = 'sentiment_category'\n",
    "\n",
    "# Find best text column to use (prefer cleaned_text, then raw text columns)\n",
    "text_col = None\n",
    "candidates = ['cleaned_text', 'text', 'tweet', 'content', 'post', 'message', 'caption', 'body']\n",
    "for col in candidates:\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    raise ValueError('No suitable text column found for TF-IDF calculation.')\n",
    "\n",
    "print(f\"Using '{text_col}' column for TF-IDF analysis\")\n",
    "\n",
    "# Prepare positive and negative groups\n",
    "positive_texts = df[df[sentiment_col] == 'positive'][text_col].fillna('').tolist()\n",
    "negative_texts = df[df[sentiment_col] == 'negative'][text_col].fillna('').tolist()\n",
    "\n",
    "print(f\"Positive texts: {len(positive_texts)}, Negative texts: {len(negative_texts)}\")\n",
    "\n",
    "# Combine for vectorizer fit\n",
    "all_texts = positive_texts + negative_texts\n",
    "labels = (['positive'] * len(positive_texts)) + (['negative'] * len(negative_texts))\n",
    "\n",
    "# Fit TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', min_df=2, max_df=0.8)\n",
    "X = vectorizer.fit_transform(all_texts)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Split back to positive/negative\n",
    "X_pos = X[:len(positive_texts)]\n",
    "X_neg = X[len(positive_texts):]\n",
    "\n",
    "# Compute mean TF-IDF per word for each group\n",
    "mean_tfidf_pos = np.asarray(X_pos.mean(axis=0)).flatten()\n",
    "mean_tfidf_neg = np.asarray(X_neg.mean(axis=0)).flatten()\n",
    "\n",
    "# Get top keywords per group\n",
    "top_n = 20\n",
    "pos_top_idx = mean_tfidf_pos.argsort()[::-1][:top_n]\n",
    "neg_top_idx = mean_tfidf_neg.argsort()[::-1][:top_n]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top TF-IDF keywords for POSITIVE sentiment:\")\n",
    "print(\"-\" * 80)\n",
    "for i in pos_top_idx:\n",
    "    print(f\"  {feature_names[i]}: {mean_tfidf_pos[i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top TF-IDF keywords for NEGATIVE sentiment:\")\n",
    "print(\"-\" * 80)\n",
    "for i in neg_top_idx:\n",
    "    print(f\"  {feature_names[i]}: {mean_tfidf_neg[i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 10 completed: Top TF-IDF keywords per sentiment generated!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc30d4",
   "metadata": {},
   "source": [
    "## Task 11: Emoji extraction + sentiment mapping\n",
    "\n",
    "Output: table of emoji → sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 11: Emoji extraction + sentiment mapping\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 11: EMOJI EXTRACTION + SENTIMENT MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "# Try to import emoji library, install if missing\n",
    "try:\n",
    "    import emoji\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    print(\"emoji library not found. Installing emoji...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'emoji'])\n",
    "    import emoji\n",
    "\n",
    "# Find raw text column (original unprocessed text with emojis)\n",
    "raw_text_col = None\n",
    "candidates = [col for col in df.columns \n",
    "              if df[col].dtype == 'object' \n",
    "              and 'cleaned' not in col \n",
    "              and '_no_stopwords' not in col \n",
    "              and 'hashtags' not in col]\n",
    "\n",
    "if candidates:\n",
    "    raw_text_col = candidates[0]\n",
    "    print(f\"Using '{raw_text_col}' column for emoji extraction\")\n",
    "else:\n",
    "    raise ValueError('No suitable raw text column found for emoji extraction.')\n",
    "\n",
    "# Function to extract emojis from text\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract all emojis from text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = str(text)\n",
    "    # Use emoji library to find emojis\n",
    "    emoji_list = [char for char in text if char in emoji.EMOJI_DATA]\n",
    "    return emoji_list\n",
    "\n",
    "# Extract emojis from raw text\n",
    "print(\"\\nExtracting emojis from text...\")\n",
    "df['emojis'] = df[raw_text_col].apply(extract_emojis)\n",
    "\n",
    "# Flatten all emojis and create sentiment mapping\n",
    "emoji_sentiment_map = {}\n",
    "emoji_counts = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    emojis_in_text = row['emojis']\n",
    "    polarity = row['polarity']\n",
    "    \n",
    "    for em in emojis_in_text:\n",
    "        if em not in emoji_sentiment_map:\n",
    "            emoji_sentiment_map[em] = []\n",
    "            emoji_counts[em] = 0\n",
    "        emoji_sentiment_map[em].append(polarity)\n",
    "        emoji_counts[em] += 1\n",
    "\n",
    "# Calculate average sentiment for each emoji\n",
    "emoji_sentiment_summary = []\n",
    "for em, sentiments in emoji_sentiment_map.items():\n",
    "    avg_sentiment = np.mean(sentiments)\n",
    "    std_sentiment = np.std(sentiments)\n",
    "    count = len(sentiments)\n",
    "    \n",
    "    # Get emoji description\n",
    "    try:\n",
    "        emoji_desc = emoji.demojize(em)\n",
    "    except:\n",
    "        emoji_desc = \"unknown\"\n",
    "    \n",
    "    emoji_sentiment_summary.append({\n",
    "        'emoji': em,\n",
    "        'description': emoji_desc,\n",
    "        'avg_polarity': avg_sentiment,\n",
    "        'std_polarity': std_sentiment,\n",
    "        'count': count\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by count\n",
    "emoji_df = pd.DataFrame(emoji_sentiment_summary)\n",
    "emoji_df = emoji_df.sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"\\nTotal unique emojis found: {len(emoji_df)}\")\n",
    "print(f\"Total emoji occurrences: {emoji_df['count'].sum()}\")\n",
    "\n",
    "# Display top emojis by frequency\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top 20 Emojis by Frequency + Sentiment Score:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(emoji_df) > 0:\n",
    "    display_df = emoji_df.head(20)[['emoji', 'description', 'avg_polarity', 'count']]\n",
    "    for idx, row in display_df.iterrows():\n",
    "        emoji_char = row['emoji']\n",
    "        desc = row['description']\n",
    "        avg_pol = row['avg_polarity']\n",
    "        count = row['count']\n",
    "        sentiment_label = \"Positive\" if avg_pol > 0.1 else (\"Negative\" if avg_pol < -0.1 else \"Neutral\")\n",
    "        print(f\"{emoji_char} {desc}: avg_polarity={avg_pol:.4f} ({sentiment_label}), count={count}\")\n",
    "else:\n",
    "    print(\"No emojis found in the dataset.\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Emoji Sentiment Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "if len(emoji_df) > 0:\n",
    "    print(f\"Average polarity of emojis: {emoji_df['avg_polarity'].mean():.4f}\")\n",
    "    print(f\"Most positive emoji avg_polarity: {emoji_df['avg_polarity'].max():.4f}\")\n",
    "    print(f\"Most negative emoji avg_polarity: {emoji_df['avg_polarity'].min():.4f}\")\n",
    "    \n",
    "    # Categorize emojis\n",
    "    positive_emojis = len(emoji_df[emoji_df['avg_polarity'] > 0.1])\n",
    "    negative_emojis = len(emoji_df[emoji_df['avg_polarity'] < -0.1])\n",
    "    neutral_emojis = len(emoji_df[(emoji_df['avg_polarity'] >= -0.1) & (emoji_df['avg_polarity'] <= 0.1)])\n",
    "    \n",
    "    print(f\"Positive emojis: {positive_emojis}\")\n",
    "    print(f\"Negative emojis: {negative_emojis}\")\n",
    "    print(f\"Neutral emojis: {neutral_emojis}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 11 completed: Emojis extracted and sentiment mapped!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12623ed3",
   "metadata": {},
   "source": [
    "## Task 12: Hashtag frequency analysis\n",
    "\n",
    "Output: table of hashtags → count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 12: Hashtag frequency analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 12: HASHTAG FREQUENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Extract and flatten all hashtags from the hashtags column\n",
    "all_hashtags = []\n",
    "for hashtag_list in df['hashtags']:\n",
    "    if isinstance(hashtag_list, list):\n",
    "        all_hashtags.extend(hashtag_list)\n",
    "\n",
    "# Count hashtag frequencies\n",
    "hashtag_counter = Counter(all_hashtags)\n",
    "\n",
    "# Create summary DataFrame\n",
    "hashtag_freq_df = pd.DataFrame(\n",
    "    list(hashtag_counter.most_common()),\n",
    "    columns=['hashtag', 'count']\n",
    ")\n",
    "\n",
    "# Add percentage column\n",
    "total_hashtags = hashtag_freq_df['count'].sum()\n",
    "hashtag_freq_df['percentage'] = (hashtag_freq_df['count'] / total_hashtags * 100).round(2)\n",
    "\n",
    "print(f\"\\nTotal unique hashtags: {len(hashtag_freq_df)}\")\n",
    "print(f\"Total hashtag occurrences: {total_hashtags}\")\n",
    "\n",
    "# Display top 30 hashtags\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top 30 Hashtags by Frequency:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_30 = hashtag_freq_df.head(30)\n",
    "for idx, row in top_30.iterrows():\n",
    "    hashtag = row['hashtag']\n",
    "    count = row['count']\n",
    "    percentage = row['percentage']\n",
    "    print(f\"{hashtag}: {count} ({percentage}%)\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Hashtag Frequency Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Most frequent hashtag: {hashtag_freq_df.iloc[0]['hashtag']} ({hashtag_freq_df.iloc[0]['count']} occurrences)\")\n",
    "print(f\"Average hashtag frequency: {hashtag_freq_df['count'].mean():.2f}\")\n",
    "print(f\"Median hashtag frequency: {hashtag_freq_df['count'].median():.2f}\")\n",
    "print(f\"Max frequency: {hashtag_freq_df['count'].max()}\")\n",
    "print(f\"Min frequency: {hashtag_freq_df['count'].min()}\")\n",
    "\n",
    "# Trending analysis: hashtags appearing more than once\n",
    "trending_threshold = 2\n",
    "trending_hashtags = hashtag_freq_df[hashtag_freq_df['count'] >= trending_threshold]\n",
    "print(f\"\\nHashtags with 2+ occurrences (trending): {len(trending_hashtags)}\")\n",
    "print(f\"Hashtags appearing only once: {len(hashtag_freq_df[hashtag_freq_df['count'] == 1])}\")\n",
    "\n",
    "# Top hashtags contribution\n",
    "top_10_contribution = top_30['count'].sum() / total_hashtags * 100\n",
    "print(f\"\\nTop 10 hashtags contribute: {top_10_contribution:.2f}% of all hashtag occurrences\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 12 completed: Hashtag frequency analysis completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897c70b",
   "metadata": {},
   "source": [
    "## Task 13: Hashtag sentiment analysis\n",
    "\n",
    "Output: `hashtag` → average sentiment table (avg_polarity, std_polarity, count, frequency, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 13: Hashtag sentiment analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 13: HASHTAG SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import os\n",
    "\n",
    "# Check that required columns exist\n",
    "if 'hashtags' not in df.columns:\n",
    "    raise ValueError(\"Column 'hashtags' not found. Run Task 6 before Task 13.\")\n",
    "if 'polarity' not in df.columns:\n",
    "    raise ValueError(\"Column 'polarity' not found. Run Task 7 before Task 13.\")\n",
    "\n",
    "# Explode hashtags to associate each hashtag with the row's polarity\n",
    "expanded = df[['hashtags', 'polarity']].explode('hashtags')\n",
    "# Drop rows with no hashtag (NaN after explode)\n",
    "expanded = expanded.dropna(subset=['hashtags'])\n",
    "\n",
    "# Normalize hashtags to consistent case (optional)\n",
    "expanded['hashtag_norm'] = expanded['hashtags'].astype(str).str.lower()\n",
    "\n",
    "# Compute aggregate sentiment per hashtag\n",
    "hashtag_sentiment = expanded.groupby('hashtag_norm')['polarity'].agg(\n",
    "    avg_polarity='mean', std_polarity='std', count='count'\n",
    ").reset_index()\n",
    "\n",
    "# If Task 12 produced `hashtag_freq_df` we can merge percentage; otherwise compute frequency from this table\n",
    "if 'hashtag_freq_df' in globals():\n",
    "    # Ensure hashtag column names align (hashtag_freq_df has 'hashtag')\n",
    "    hashtag_sentiment = hashtag_sentiment.merge(\n",
    "        hashtag_freq_df.rename(columns={'hashtag':'hashtag_norm'}),\n",
    "        on='hashtag_norm', how='left'\n",
    "    )\n",
    "else:\n",
    "    total_hashtags = hashtag_sentiment['count'].sum()\n",
    "    hashtag_sentiment['percentage'] = (hashtag_sentiment['count'] / total_hashtags * 100).round(2)\n",
    "\n",
    "# Sort by count descending\n",
    "hashtag_sentiment = hashtag_sentiment.sort_values('count', ascending=False)\n",
    "\n",
    "# Create output directory and save CSV\n",
    "out_dir = '../data/processed'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_file = os.path.join(out_dir, 'hashtag_sentiment.csv')\n",
    "hashtag_sentiment.to_csv(out_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nSaved hashtag sentiment table to: {out_file}\")\n",
    "print(f\"Total hashtags in table: {len(hashtag_sentiment)}\")\n",
    "\n",
    "# Print top 30 hashtags with sentiment\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top 30 hashtags with average polarity and counts:\")\n",
    "print(\"-\" * 80)\n",
    "print(hashtag_sentiment.head(30).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 13 completed: Hashtag sentiment table created and exported.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb339ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 15: Filter only negative sentiment records\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 15: FILTER NEGATIVE SENTIMENT RECORDS -> negative_df')\n",
    "print('=' * 80)\n",
    "\n",
    "import os\n",
    "\n",
    "# Ensure polarity column exists\n",
    "if 'polarity' not in df.columns:\n",
    "    raise ValueError(\"Column 'polarity' not found. Run Task 7 to compute sentiment polarity before Task 15.\")\n",
    "\n",
    "# Ensure sentiment_category exists; compute if missing using same thresholds as Task 9\n",
    "positive_threshold = 0.1\n",
    "negative_threshold = -0.1\n",
    "if 'sentiment_category' not in df.columns:\n",
    "    print(\"'sentiment_category' not found — computing from 'polarity' using thresholds.\")\n",
    "    df['sentiment_category'] = df['polarity'].apply(\n",
    "        lambda x: 'positive' if x > positive_threshold else ('negative' if x < negative_threshold else 'neutral')\n",
    "    )\n",
    "\n",
    "# Filter negatives\n",
    "negative_df = df[df['sentiment_category'] == 'negative'].copy()\n",
    "\n",
    "print(f\"Negative records: {len(negative_df)} rows\")\n",
    "if len(negative_df) > 0:\n",
    "    print('\\nSample negative rows (first 5):')\n",
    "    print(negative_df.head(5).to_string(index=False))\n",
    "else:\n",
    "    print('No negative records found.')\n",
    "\n",
    "# Optionally save to CSV for downstream use\n",
    "out_dir = '../data/processed'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "neg_path = os.path.join(out_dir, 'negative_df.csv')\n",
    "negative_df.to_csv(neg_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nSaved negative_df to: {neg_path}\")\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 15 completed: `negative_df` created and exported.')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c629f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 16: Extract negative n-grams (bigrams & trigrams)\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 16: NEGATIVE BIGRAM & TRIGRAM FREQUENCY')\n",
    "print('=' * 80)\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we have a negative dataframe (negative_df) or compute it\n",
    "if 'negative_df' not in globals():\n",
    "    print('`negative_df` not found in memory — computing from `df`.')\n",
    "    if 'polarity' not in df.columns:\n",
    "        raise ValueError(\"Column 'polarity' not found. Run sentiment Task 7 first.\")\n",
    "    # use same thresholds as Task 9\n",
    "    pos_thr = 0.1\n",
    "    neg_thr = -0.1\n",
    "    if 'sentiment_category' not in df.columns:\n",
    "        df['sentiment_category'] = df['polarity'].apply(lambda x: 'positive' if x>pos_thr else ('negative' if x<neg_thr else 'neutral'))\n",
    "    negative_df = df[df['sentiment_category'] == 'negative'].copy()\n",
    "\n",
    "print(f'Negative rows to process: {len(negative_df)}')\n",
    "\n",
    "# Pick text column preference (prefer tokenized/no-stopwords)\n",
    "candidates = ['text_no_stopwords','cleaned_text','text','tweet','content','post','message','caption','body']\n",
    "text_col = None\n",
    "for c in candidates:\n",
    "    if c in negative_df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    print('⚠ No suitable text column found in negative_df. Skipping n-gram extraction.')\n",
    "else:\n",
    "    print(f'Using `{text_col}` for n-gram extraction')\n",
    "    texts = negative_df[text_col].fillna('').astype(str).tolist()\n",
    "    bigram_counter = Counter()\n",
    "    trigram_counter = Counter()\n",
    "\n",
    "    for t in texts:\n",
    "        tokens = t.split()\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        bigrams = ngrams(tokens, 2)\n",
    "        trigrams = ngrams(tokens, 3) if len(tokens) >= 3 else []\n",
    "        bigram_counter.update([' '.join(g) for g in bigrams])\n",
    "        trigram_counter.update([' '.join(g) for g in trigrams])\n",
    "\n",
    "    # Create DataFrames and export top N\n",
    "    top_n = 200\n",
    "    neg_bigram_df = pd.DataFrame(bigram_counter.most_common(top_n), columns=['bigram','frequency'])\n",
    "    neg_trigram_df = pd.DataFrame(trigram_counter.most_common(top_n), columns=['trigram','frequency'])\n",
    "\n",
    "    out_dir = '../data/processed'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    bigram_path = os.path.join(out_dir, 'negative_bigrams.csv')\n",
    "    trigram_path = os.path.join(out_dir, 'negative_trigrams.csv')\n",
    "    neg_bigram_df.to_csv(bigram_path, index=False, encoding='utf-8')\n",
    "    neg_trigram_df.to_csv(trigram_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print('\n",
    "Top 30 Negative Bigrams:')\n",
    "    for gram, freq in neg_bigram_df.head(30).values:\n",
    "        print(f'  {gram}: {freq}')\n",
    "\n",
    "    print('\n",
    "Top 30 Negative Trigrams:')\n",
    "    for gram, freq in neg_trigram_df.head(30).values:\n",
    "        print(f'  {gram}: {freq}')\n",
    "\n",
    "    print(f'\\nSaved negative bigrams to: {bigram_path}')\n",
    "    print(f'Saved negative trigrams to: {trigram_path}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 16 completed: negative n-gram frequency tables created and exported.')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 17: Topic modeling (LDA default, KMeans optional) on negative posts\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 17: TOPIC MODELING ON NEGATIVE POSTS')\n",
    "print('=' * 80)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Parameters - adjust if needed\n",
    "n_topics = 8\n",
    "top_n_words = 12\n",
    "method = 'lda'  # choices: 'lda' or 'kmeans'\n",
    "out_dir = '../data/processed'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Ensure negative_df exists (compute if necessary)\n",
    "if 'negative_df' not in globals():\n",
    "    print('`negative_df` not in memory — deriving from `df`')\n",
    "    if 'polarity' not in df.columns:\n",
    "        raise ValueError('Column \n",
    " not found. Run Task 7 first.')\n",
    "    pos_thr = 0.1\n",
    "    neg_thr = -0.1\n",
    "    if 'sentiment_category' not in df.columns:\n",
    "        df['sentiment_category'] = df['polarity'].apply(lambda x: 'positive' if x>pos_thr else ('negative' if x<neg_thr else 'neutral'))\n",
    "    negative_df = df[df['sentiment_category'] == 'negative'].copy()\n",
    "\n",
    "print(f'Negative documents: {len(negative_df)}')\n",
    "if len(negative_df) == 0:\n",
    "    print('No negative documents to model. Skipping Task 17.')\n",
    "else:\n",
    "    # Choose text column\n",
    "    candidates = ['text_no_stopwords','cleaned_text','text','tweet','content','post','message','caption','body']\n",
    "    text_col = next((c for c in candidates if c in negative_df.columns), None)\n",
    "    if text_col is None:\n",
    "        raise ValueError('No suitable text column found for topic modeling.')\n",
    "    print(f'Using `{text_col}` for topic modeling')\n",
    "    docs = negative_df[text_col].fillna('').astype(str).tolist()\n",
    "\n",
    "    try:\n",
    "        if method == 'lda':\n",
    "            print('Building document-term matrix (CountVectorizer)')\n",
    "            vectorizer = CountVectorizer(max_df=0.95, min_df=3, max_features=5000, stop_words='english')\n",
    "            dtm = vectorizer.fit_transform(docs)\n",
    "            print('Fitting LDA model...')\n",
    "            lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method='batch', max_iter=20)\n",
    "            lda.fit(dtm)\n",
    "            # Extract topic keywords\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            topics = []\n",
    "            for topic_idx, topic in enumerate(lda.components_):\n",
    "                top_features_ind = topic.argsort()[::-1][:top_n_words]\n",
    "                top_features = [feature_names[i] for i in top_features_ind]\n",
    "                topics.append({'topic': topic_idx, 'keywords': ' '.join(top_features)})\n",
    "            topics_df = pd.DataFrame(topics)\n",
    "            # Assign dominant topic to each document\n",
    "            doc_topic_dist = lda.transform(dtm)\n",
    "            doc_topics = doc_topic_dist.argmax(axis=1)\n",
    "            negative_df['topic'] = ['topic_' + str(t) for t in doc_topics]\n",
    "        else:\n",
    "            # KMeans on TF-IDF\n",
    "            print('Computing TF-IDF and running KMeans...')\n",
    "            tfv = TfidfVectorizer(max_df=0.95, min_df=3, max_features=5000, stop_words='english')\n",
    "            X = tfv.fit_transform(docs)\n",
    "            kmeans = KMeans(n_clusters=n_topics, random_state=42, n_init=10)\n",
    "            kmeans.fit(X)\n",
    "            labels = kmeans.labels_\n",
    "            negative_df['topic'] = ['topic_' + str(t) for t in labels]\n",
    "            # extract top terms per cluster by centroid\n",
    "            try:\n",
    "                terms = tfv.get_feature_names_out()\n",
    "                order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "                topics = []\n",
    "                for i in range(n_topics):\n",
    "                    top_terms = [terms[ind] for ind in order_centroids[i, :top_n_words]]\n",
    "                    topics.append({'topic': i, 'keywords': ' '.join(top_terms)})\n",
    "                topics_df = pd.DataFrame(topics)\n",
    "            except Exception:\n",
    "                topics_df = pd.DataFrame([])\n",
    "        # Save outputs\n",
    "        topics_path = os.path.join(out_dir, 'negative_topics_keywords.csv')\n",
    "        topics_df.to_csv(topics_path, index=False, encoding='utf-8')\n",
    "        neg_topics_path = os.path.join(out_dir, 'negative_df_topics.csv')\n",
    "        negative_df.to_csv(neg_topics_path, index=False, encoding='utf-8')\n",
    "        print(f'✓ Saved topic keywords to: {topics_path}')\n",
    "        print(f'✓ Saved negative_df with topic labels to: {neg_topics_path}')\n",
    "        print('\n",
    "Top topics (keywords):')\n",
    "        print(topics_df.head(20).to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print('⚠ Topic modeling failed:', e)\n",
    "\n",
    "print('\n",
    "' + '=' * 80)\n",
    "print('✓ Task 17 completed: Topic modeling executed and outputs saved (if successful).')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 18: Summarize topic clusters\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 18: SUMMARIZE TOPIC CLUSTERS')\n",
    "print('=' * 80)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "out_dir = '../data/processed'\n",
    "topics_path = os.path.join(out_dir, 'negative_topics_keywords.csv')\n",
    "\n",
    "# Load topics_df from memory or CSV\n",
    "if 'topics_df' not in globals():\n",
    "    if os.path.exists(topics_path):\n",
    "        topics_df = pd.read_csv(topics_path)\n",
    "        print(f\\\n",
    "    else:\n",
    "        raise ValueError(\\\n",
    "17\n",
    ")\n",
    "\n",
    "# Ensure keywords column is available\n",
    "if 'keywords' not in topics_df.columns:\n",
    "    raise ValueError(\\\n",
    ")\n",
    "\n",
    "# Build summary with cluster_name and key_terms\n",
    "def format_cluster_name(t):\n",
    "    try:\n",
    "        return f\\\n",
    "\n",
    "    except Exception:\n",
    "        return str(t)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'cluster_name': topics_df['topic'].apply(format_cluster_name) if 'topic' in topics_df.columns else ['cluster_' + str(i) for i in topics_df.index],\n",
    "    'key_terms': topics_df['keywords']\n",
    "})\n",
    "\n",
    "# Save summary\n",
    "summary_path = os.path.join(out_dir, 'negative_topics_summary.csv')\n",
    "summary_df.to_csv(summary_path, index=False, encoding='utf-8')\n",
    "print(f\\\n",
    "print('\\nTop clusters (name + key terms):')\n",
    "print(summary_df.head(50).to_string(index=False))\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 18 completed: Topic clusters summarized and exported.')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 19: Identify toxic keywords\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 19: IDENTIFY TOXIC KEYWORDS')\n",
    "print('=' * 80)\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "out_dir = '../data/processed'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "# Prefer a supplied lexicon in processed/; fall back to a small default list\n",
    "lex_csv = os.path.join(out_dir, 'toxicity_lexicon.csv')\n",
    "lex_txt = os.path.join(out_dir, 'toxicity_lexicon.txt')\n",
    "toxicity_lexicon = None\n",
    "if os.path.exists(lex_csv):\n",
    "    try:\n",
    "        toxicity_lexicon = pd.read_csv(lex_csv, header=None).iloc[:,0].astype(str).str.lower().str.strip().dropna().unique().tolist()\n",
    "        print(f'Loaded toxicity lexicon from: {lex_csv}')\n",
    "    except Exception as e:\n",
    "        print('Failed to read lexicon CSV:', e)\n",
    "elif os.path.exists(lex_txt):\n",
    "    try:\n",
    "        with open(lex_txt, 'r', encoding='utf-8') as f:\n",
    "            toxicity_lexicon = [l.strip().lower() for l in f if l.strip()]\n",
    "        print(f'Loaded toxicity lexicon from: {lex_txt}')\n",
    "    except Exception as e:\n",
    "        print('Failed to read lexicon TXT:', e)\n",
    "else:\n",
    "    # Minimal example lexicon; replace with a fuller lexicon file for production\n",
    "    toxicity_lexicon = ['insult', 'hate', 'scam', 'fraud', 'abuse', 'spam', 'stupid', 'idiot', 'terrible', 'awful']\n",
    "    print('No lexicon file found — using built-in minimal lexicon (recommend providing a larger list at data/processed/toxicity_lexicon.csv)')\n",
    "\n",
    "# Choose text column (prefer tokenized/no-stopwords)\n",
    "candidates = ['text_no_stopwords','cleaned_text','text','tweet','content','post','message','caption','body']\n",
    "text_col = next((c for c in candidates if c in globals().get('df', {}) and c in df.columns), None)\n",
    "if text_col is None:\n",
    "    # fall back to checking df columns directly\n",
    "    text_col = next((c for c in candidates if c in df.columns), None)\n",
    "if text_col is None:\n",
    "    raise ValueError('No suitable text column found for toxicity detection. Create `cleaned_text` or `text_no_stopwords` first.')\n",
    "print(f'Using \n",
    " as source for toxicity counting')\n",
    "\n",
    "# Prepare pattern list (word-boundary safe)\n",
    "tox_patterns = [re.compile(r'\\b' + re.escape(w) + r'\\b', flags=re.IGNORECASE) for w in toxicity_lexicon]\n",
    "\n",
    "# Count occurrences across documents\n",
    "counter = Counter()\n",
    "rows_with_toxic = 0\n",
    "for text in df[text_col].fillna('').astype(str):\n",
    "    text_lower = text.lower()\n",
    "    found_any = False\n",
    "    for w, pat in zip(toxicity_lexicon, tox_patterns):\n",
    "        matches = len(pat.findall(text_lower))\n",
    "        if matches > 0:\n",
    "            counter[w] += matches\n",
    "            found_any = True\n",
    "    if found_any:\n",
    "        rows_with_toxic += 1\n",
    "\n",
    "# Build DataFrame of frequencies\n",
    "tox_df = pd.DataFrame([(w, counter.get(w, 0)) for w in toxicity_lexicon], columns=['toxic_word','frequency'])\n",
    "tox_df = tox_df.sort_values('frequency', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save output\n",
    "out_file = os.path.join(out_dir, 'toxic_words_frequency.csv')\n",
    "tox_df.to_csv(out_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'\n",
    "Saved toxic-word frequency table to: {out_file}')\n",
    "print(f'Total rows scanned: {len(df)}; rows containing >=1 toxic word: {rows_with_toxic}')\n",
    "print('\n",
    "Top toxic words by frequency:')\n",
    "print(tox_df[tox_df['frequency']>0].head(50).to_string(index=False))\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 19 completed: Toxic keywords identified and frequencies exported.')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5995e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 21: Create Power BI data model (export core tables)\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 21: CREATE POWER BI DATA MODEL - EXPORT TABLES')\n",
    "print('=' * 80)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "out_dir = '../data/processed'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "saved = []\n",
    "# 1) clean_data\n",
    "if 'df' in globals():\n",
    "    clean_path = os.path.join(out_dir, 'clean_data.csv')\n",
    "    df.to_csv(clean_path, index=False, encoding='utf-8')\n",
    "    saved.append(clean_path)\n",
    "    print(f'Saved clean_data to: {clean_path}')\n",
    "else:\n",
    "    print('Warning: `df` not present in memory; skipping clean_data export.')\n",
    "\n",
    "# 2) hashtags table (frequency + sentiment if available)\n",
    "try:\n",
    "    if 'hashtag_freq_df' in globals():\n",
    "        hashtags_df = hashtag_freq_df.copy()\n",
    "    elif 'hashtag_sentiment' in globals():\n",
    "        hashtags_df = hashtag_sentiment.rename(columns={'hashtag_norm':'hashtag'})\n",
    "    else:\n",
    "        # derive from df['hashtags']\n",
    "        from collections import Counter\n",
    "        all_hashtags = []\n",
    "        if 'hashtags' in df.columns:\n",
    "            for lst in df['hashtags']:\n",
    "                if isinstance(lst, list):\n",
    "                    all_hashtags.extend(lst)\n",
    "        counter = Counter(all_hashtags)\n",
    "        hashtags_df = pd.DataFrame(list(counter.items()), columns=['hashtag','count']).sort_values('count', ascending=False)\n",
    "        total = hashtags_df['count'].sum() if len(hashtags_df)>0 else 0\n",
    "        hashtags_df['percentage'] = (hashtags_df['count'] / total * 100).round(2) if total>0 else 0\n",
    "    hashtags_path = os.path.join(out_dir, 'hashtags.csv')\n",
    "    hashtags_df.to_csv(hashtags_path, index=False, encoding='utf-8')\n",
    "    saved.append(hashtags_path)\n",
    "    print(f'Saved hashtags table to: {hashtags_path}')\n",
    "except Exception as e:\n",
    "    print('Failed to build/save hashtags table:', e)\n",
    "\n",
    "# 3) sentiment drivers (keyword_drivers) - prefer existing TF-IDF results else compute lightweight version\n",
    "try:\n",
    "    kw_path = os.path.join(out_dir, 'keyword_drivers.csv')\n",
    "    if 'feature_names' in globals() and 'mean_tfidf_pos' in globals() and 'mean_tfidf_neg' in globals():\n",
    "        import numpy as _np\n",
    "        kws = list(feature_names)\n",
    "        pos_scores = _np.round(mean_tfidf_pos, 6).tolist()\n",
    "        neg_scores = _np.round(mean_tfidf_neg, 6).tolist()\n",
    "        kd = pd.DataFrame({'keyword': kws, 'tfidf_pos': pos_scores, 'tfidf_neg': neg_scores})\n",
    "        kd['score_diff'] = (kd['tfidf_pos'] - kd['tfidf_neg']).abs()\n",
    "        kd = kd.sort_values('score_diff', ascending=False)\n",
    "        kd.to_csv(kw_path, index=False, encoding='utf-8')\n",
    "        saved.append(kw_path)\n",
    "        print(f'Saved keyword drivers to: {kw_path} (from existing TF-IDF)')\n",
    "    else:\n",
    "        # lightweight recompute (may be slower) - use cleaned_text or text_no_stopwords\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        text_candidates = ['cleaned_text','text_no_stopwords','text','tweet','content']\n",
    "        tcol = next((c for c in text_candidates if c in df.columns), None)\n",
    "        if tcol is None:\n",
    "            raise ValueError('No text column for TF-IDF computation')\n",
    "        pos_texts = df[df.get('sentiment_category','')=='positive'][tcol].fillna('').tolist()\n",
    "        neg_texts = df[df.get('sentiment_category','')=='negative'][tcol].fillna('').tolist()\n",
    "        all_texts = pos_texts + neg_texts\n",
    "        if len(all_texts)==0:\n",
    "            raise ValueError('No texts available for TF-IDF')\n",
    "        vec = TfidfVectorizer(max_features=1000, stop_words='english', min_df=2, max_df=0.8)\n",
    "        X = vec.fit_transform(all_texts)\n",
    "        fnames = vec.get_feature_names_out()\n",
    "        import numpy as _np\n",
    "        X_pos = X[:len(pos_texts)] if len(pos_texts)>0 else _np.zeros((0, X.shape[1]))\n",
    "        X_neg = X[len(pos_texts):] if len(neg_texts)>0 else _np.zeros((0, X.shape[1]))\n",
    "        mean_pos = _np.asarray(X_pos.mean(axis=0)).flatten() if X_pos.shape[0]>0 else _np.zeros(X.shape[1])\n",
    "        mean_neg = _np.asarray(X_neg.mean(axis=0)).flatten() if X_neg.shape[0]>0 else _np.zeros(X.shape[1])\n",
    "        kd = pd.DataFrame({'keyword': list(fnames), 'tfidf_pos': _np.round(mean_pos,6), 'tfidf_neg': _np.round(mean_neg,6)})\n",
    "        kd['score_diff'] = (kd['tfidf_pos'] - kd['tfidf_neg']).abs()\n",
    "        kd = kd.sort_values('score_diff', ascending=False)\n",
    "        kd.to_csv(kw_path, index=False, encoding='utf-8')\n",
    "        saved.append(kw_path)\n",
    "        print(f'Saved keyword drivers to: {kw_path} (recomputed TF-IDF)')\n",
    "except Exception as e:\n",
    "    print('Failed to build/save keyword drivers:', e)\n",
    "\n",
    "# 4) negative analysis bundle: negative_df, negative n-grams, topic summary\n",
    "try:\n",
    "    # negative_df\n",
    "    if 'negative_df' in globals():\n",
    "        neg = negative_df.copy()\n",
    "    else:\n",
    "        if 'polarity' not in df.columns:\n",
    "            raise ValueError('No polarity column to derive negative_df')\n",
    "        pos_thr = 0.1; neg_thr = -0.1\n",
    "        if 'sentiment_category' not in df.columns:\n",
    "            df['sentiment_category'] = df['polarity'].apply(lambda x: 'positive' if x>pos_thr else ('negative' if x<neg_thr else 'neutral'))\n",
    "        neg = df[df['sentiment_category']=='negative'].copy()\n",
    "    neg_path = os.path.join(out_dir, 'negative_df.csv')\n",
    "    neg.to_csv(neg_path, index=False, encoding='utf-8')\n",
    "    saved.append(neg_path)\n",
    "    print(f'Saved negative_df to: {neg_path}')\n",
    "    # negative n-grams (use existing if present)\n",
    "    if 'neg_bigram_df' in globals():\n",
    "        neg_bigram_df.to_csv(os.path.join(out_dir, 'negative_bigrams.csv'), index=False, encoding='utf-8')\n",
    "        saved.append(os.path.join(out_dir, 'negative_bigrams.csv'))\n",
    "    if 'neg_trigram_df' in globals():\n",
    "        neg_trigram_df.to_csv(os.path.join(out_dir, 'negative_trigrams.csv'), index=False, encoding='utf-8')\n",
    "        saved.append(os.path.join(out_dir, 'negative_trigrams.csv'))\n",
    "    # topic summary if exists\n",
    "    topic_sum_path = os.path.join(out_dir, 'negative_topics_summary.csv')\n",
    "    if os.path.exists(topic_sum_path):\n",
    "        saved.append(topic_sum_path)\n",
    "    elif 'topics_df' in globals():\n",
    "        td = topics_df.copy()\n",
    "        if 'topic' in td.columns:\n",
    "            td['cluster_name'] = td['topic'].apply(lambda t: 'cluster_' + str(t))\n",
    "        td = td.rename(columns={'keywords':'key_terms'})[['cluster_name','key_terms']]\n",
    "        td.to_csv(topic_sum_path, index=False, encoding='utf-8')\n",
    "        saved.append(topic_sum_path)\n",
    "        print(f'Saved negative topic summary to: {topic_sum_path}')\n",
    "except Exception as e:\n",
    "    print('Failed to build/save negative analysis artifacts:', e)\n",
    "\n",
    "print('\\nExport summary:')\n",
    "for p in saved:\n",
    "    try:\n",
    "        size_mb = os.path.getsize(p)/(1024*1024)\n",
    "        print(f'  - {p} ({size_mb:.2f} MB)')\n",
    "    except Exception:\n",
    "        print(f'  - {p}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 21 completed: Power BI tables exported to data/processed')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 22: Build Overview page - sentiment distribution, time-series trend, word cloud\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 22: OVERVIEW VISUALS (SENTIMENT DISTRIBUTION, TREND, WORDCLOUD)')\n",
    "print('=' * 80)\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "out_dir = '../data/processed'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Ensure df exists\n",
    "if 'df' not in globals():\n",
    "    raise ValueError('`df` not loaded in memory. Run Task 1-8 to prepare `df`.')\n",
    "\n",
    "# 1) Sentiment distribution\n",
    "if 'sentiment_category' not in df.columns:\n",
    "    # attempt to compute from polarity column\n",
    "    if 'polarity' in df.columns:\n",
    "        pos_thr, neg_thr = 0.1, -0.1\n",
    "        df['sentiment_category'] = df['polarity'].apply(lambda x: 'positive' if x>pos_thr else ('negative' if x<neg_thr else 'neutral'))\n",
    "    else:\n",
    "        raise ValueError('No sentiment information available. Run Task 7 to compute polarity.')\n",
    "\n",
    "sent_counts = df['sentiment_category'].value_counts().reindex(['positive','neutral','negative']).fillna(0)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=sent_counts.index, y=sent_counts.values, palette=['green','gray','red'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Sentiment')\n",
    "sent_img = os.path.join(out_dir, 'overview_sentiment_distribution.png')\n",
    "plt.tight_layout()\n",
    "plt.savefig(sent_img, dpi=150)\n",
    "plt.show()\n",
    "print(f'Saved sentiment distribution image to: {sent_img}')\n",
    "\n",
    "# 2) Time-series trend (by date)\n",
    "# Find a date column\n",
    "date_candidates = ['date','created_at','timestamp','created','post_date']\n",
    "date_col = next((c for c in date_candidates if c in df.columns), None)\n",
    "if date_col is None:\n",
    "    # try to infer any datetime-like column\n",
    "    for c in df.columns:\n",
    "        if 'date' in c.lower() or 'time' in c.lower():\n",
    "            date_col = c; break\n",
    "\n",
    "if date_col is None:\n",
    "    print('No date column found — skipping time-series trend.')\n",
    "else:\n",
    "    dt = df[[date_col,'sentiment_category']].copy()\n",
    "    dt[date_col] = pd.to_datetime(dt[date_col], errors='coerce')\n",
    "    dt = dt.dropna(subset=[date_col])\n",
    "    # Resample daily counts per sentiment\n",
    "    dt.set_index(date_col, inplace=True)\n",
    "    daily = dt.groupby([pd.Grouper(freq='D'), 'sentiment_category']).size().unstack(fill_value=0)\n",
    "    # Plot trend (last 90 days if long)\n",
    "    plot_df = daily.tail(180)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for col in plot_df.columns:\n",
    "        plt.plot(plot_df.index, plot_df[col], label=col)\n",
    "    plt.legend()\n",
    "    plt.title('Sentiment Trend Over Time')\n",
    "    plt.ylabel('Daily Count')\n",
    "    plt.xlabel('Date')\n",
    "    plt.xticks(rotation=45)\n",
    "    trend_img = os.path.join(out_dir, 'overview_sentiment_trend.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(trend_img, dpi=150)\n",
    "    plt.show()\n",
    "    print(f'Saved sentiment trend image to: {trend_img}')\n",
    "\n",
    "# 3) Word cloud (from text_no_stopwords or cleaned_text)\n",
    "text_col = next((c for c in ['text_no_stopwords','cleaned_text','text','tweet','content'] if c in df.columns), None)\n",
    "if text_col is None:\n",
    "    print('No suitable text column for word cloud — skipping.')\n",
    "else:\n",
    "    all_text = ' '.join(df[text_col].fillna('').astype(str).tolist())\n",
    "    if not all_text.strip():\n",
    "        print('Text column empty — skipping word cloud.')\n",
    "    else:\n",
    "        stopwords = set(STOPWORDS)\n",
    "        wc = WordCloud(width=1200, height=600, background_color='white', stopwords=stopwords, collocations=True).generate(all_text)\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        wc_img = os.path.join(out_dir, 'overview_wordcloud.png')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(wc_img, dpi=150)\n",
    "        plt.show()\n",
    "        print(f'Saved word cloud image to: {wc_img}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 22 completed: Overview visuals generated and saved to data/processed')\n",
    "print('=' * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
