{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e000931f-cc9d-4d80-94f1-134a7196a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET SCHEMA INFORMATION\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: (5000, 12)\n",
      "  - Rows: 5000\n",
      "  - Columns: 12\n",
      "\n",
      "================================================================================\n",
      "Dataset Info:\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Post_ID           5000 non-null   object        \n",
      " 1   Post_Date         5000 non-null   datetime64[ns]\n",
      " 2   Platform          5000 non-null   object        \n",
      " 3   Hashtag           5000 non-null   object        \n",
      " 4   Content_Type      5000 non-null   object        \n",
      " 5   Region            5000 non-null   object        \n",
      " 6   Views             5000 non-null   int64         \n",
      " 7   Likes             5000 non-null   int64         \n",
      " 8   Shares            5000 non-null   int64         \n",
      " 9   Comments          5000 non-null   int64         \n",
      " 10  Engagement_Level  5000 non-null   object        \n",
      " 11  Text              5000 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(4), object(7)\n",
      "memory usage: 468.9+ KB\n",
      "\n",
      "================================================================================\n",
      "First 5 Rows:\n",
      "================================================================================\n",
      "  Post_ID  Post_Date   Platform     Hashtag Content_Type     Region    Views  \\\n",
      "0  Post_1 2022-01-13     TikTok  #Challenge        Video         UK  4163464   \n",
      "1  Post_2 2022-05-13  Instagram  #Education       Shorts      India  4155940   \n",
      "2  Post_3 2022-01-07    Twitter  #Challenge        Video     Brazil  3666211   \n",
      "3  Post_4 2022-12-05    YouTube  #Education       Shorts  Australia   917951   \n",
      "4  Post_5 2023-03-23     TikTok      #Dance         Post     Brazil    64866   \n",
      "\n",
      "    Likes  Shares  Comments Engagement_Level  \\\n",
      "0  339431   53135     19346             High   \n",
      "1  215240   65860     27239           Medium   \n",
      "2  327143   39423     36223           Medium   \n",
      "3  127125   11687     36806              Low   \n",
      "4  171361   69581      6376           Medium   \n",
      "\n",
      "                                 Text  \n",
      "0             I regret choosing this.  \n",
      "1             I regret choosing this.  \n",
      "2  It failed to meet my expectations.  \n",
      "3  I expected more from this product.  \n",
      "4            This product is amazing!  \n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load the dataset & print schema\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = 'E:/internship project/data/processed/dataset_with_text_column.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET SCHEMA INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"  - Rows: {df.shape[0]}\")\n",
    "print(f\"  - Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"First 5 Rows:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2ce655-3399-4adb-9748-6292f133133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 2: DETECT AND REMOVE DUPLICATES\n",
      "================================================================================\n",
      "\n",
      "Number of duplicate rows (before removal): 0\n",
      "\n",
      "Duplicate rows by all columns: 0\n",
      "\n",
      "Dataset shape before removing duplicates: (5000, 12)\n",
      "Dataset shape after removing duplicates: (5000, 12)\n",
      "\n",
      "Total duplicates removed: 0\n",
      "\n",
      "================================================================================\n",
      "✓ Duplicates removed successfully. Dataset now has 5000 unique rows.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Detect and remove duplicates\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 2: DETECT AND REMOVE DUPLICATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for duplicates before removal\n",
    "duplicates_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows (before removal): {duplicates_count}\")\n",
    "\n",
    "# Get duplicate rows by all columns\n",
    "print(f\"\\nDuplicate rows by all columns: {df.duplicated().sum()}\")\n",
    "\n",
    "# Get duplicate rows by specific columns (if needed to check specific column duplicates)\n",
    "print(f\"\\nDataset shape before removing duplicates: {df.shape}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "print(f\"Dataset shape after removing duplicates: {df_cleaned.shape}\")\n",
    "\n",
    "# Count of removed duplicates\n",
    "removed_count = df.shape[0] - df_cleaned.shape[0]\n",
    "print(f\"\\nTotal duplicates removed: {removed_count}\")\n",
    "\n",
    "# Update df to the cleaned version\n",
    "df = df_cleaned\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Duplicates removed successfully. Dataset now has {df.shape[0]} unique rows.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c4c429-e036-47b8-89f5-cce58973070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 3: HANDLE MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing Value Report:\n",
      "--------------------------------------------------------------------------------\n",
      "          Column  Missing_Count  Missing_Percentage\n",
      "         Post_ID              0                 0.0\n",
      "       Post_Date              0                 0.0\n",
      "        Platform              0                 0.0\n",
      "         Hashtag              0                 0.0\n",
      "    Content_Type              0                 0.0\n",
      "          Region              0                 0.0\n",
      "           Views              0                 0.0\n",
      "           Likes              0                 0.0\n",
      "          Shares              0                 0.0\n",
      "        Comments              0                 0.0\n",
      "Engagement_Level              0                 0.0\n",
      "            Text              0                 0.0\n",
      "\n",
      "Total missing values: 0\n",
      "Total cells: 60000\n",
      "Overall missing percentage: 0.0%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Handling missing values:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Verification after handling missing values:\n",
      "--------------------------------------------------------------------------------\n",
      "Total remaining missing values: 0\n",
      "✓ All missing values have been handled successfully!\n",
      "\n",
      "================================================================================\n",
      "✓ Missing values handled. Dataset now has 5000 rows and 12 columns.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# TASK 3: HANDLE MISSING VALUES\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 3: HANDLE MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate missing value report\n",
    "print(\"\\nMissing Value Report:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_report = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(missing_report.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_percentage = (total_missing / total_cells * 100).round(2)\n",
    "\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n",
    "print(f\"Total cells: {total_cells}\")\n",
    "print(f\"Overall missing percentage: {missing_percentage}%\")\n",
    "\n",
    "# Handle missing values based on data type\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Handling missing values:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_filled = df.copy()\n",
    "\n",
    "# For numeric columns: fill with median\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "for col in numeric_cols:\n",
    "    if df_filled[col].isnull().sum() > 0:\n",
    "        median_val = df_filled[col].median()\n",
    "        df_filled[col].fillna(median_val, inplace=True)\n",
    "        print(f\"  ✓ {col}: filled {df[col].isnull().sum()} missing values with median ({median_val:.2f})\")\n",
    "\n",
    "# For categorical columns: fill with mode\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_filled[col].isnull().sum() > 0:\n",
    "        mode_val = df_filled[col].mode()[0]\n",
    "        df_filled[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"  ✓ {col}: filled {df[col].isnull().sum()} missing values with mode ('{mode_val}')\")\n",
    "\n",
    "# Verify missing values after handling\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verification after handling missing values:\")\n",
    "print(\"-\" * 80)\n",
    "remaining_missing = df_filled.isnull().sum().sum()\n",
    "print(f\"Total remaining missing values: {remaining_missing}\")\n",
    "\n",
    "if remaining_missing == 0:\n",
    "    print(\"✓ All missing values have been handled successfully!\")\n",
    "else:\n",
    "    print(f\"⚠ Still {remaining_missing} missing values remaining\")\n",
    "\n",
    "# Update df to the cleaned version\n",
    "df = df_filled\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Missing values handled. Dataset now has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3e44b8d-f973-46c8-8d54-d3a59d24117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 4 (single column): TEXT NORMALIZATION -> cleaned_text\n",
      "================================================================================\n",
      "Using primary text column: Hashtag -> creating 'cleaned_text'.\n",
      "\n",
      "Dataset shape after adding 'cleaned_text': (5000, 14)\n",
      "\n",
      "Sample cleaned_text (first 5 rows):\n",
      "challenge\n",
      "education\n",
      "challenge\n",
      "education\n",
      "    dance\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 4 (single cleaned_text column)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 4 (single column): TEXT NORMALIZATION -> cleaned_text\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text according to requirements:\n",
    "    - lowercase\n",
    "    - remove URLs\n",
    "    - remove punctuations\n",
    "    - remove numbers\n",
    "    - remove extra spaces\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|https\\S+', '', text)\n",
    "    # remove punctuation (keep alphanumeric and spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # replace underscores (from \\w) and collapse spaces\n",
    "    text = text.replace('_', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Choose a primary raw text column to create `cleaned_text` from\n",
    "candidates = [col for col in df.columns \n",
    "              if df[col].dtype == 'object' \n",
    "              and '_normalized' not in col \n",
    "              and '_no_stopwords' not in col \n",
    "              and '_hashtags' not in col]\n",
    "\n",
    "if candidates:\n",
    "    primary_text_col = \"Hashtag\"\n",
    "    print(f\"Using primary text column: {primary_text_col} -> creating 'cleaned_text'.\")\n",
    "    df['cleaned_text'] = df[primary_text_col].apply(normalize_text)\n",
    "    print(f\"\\nDataset shape after adding 'cleaned_text': {df.shape}\")\n",
    "    print(\"\\nSample cleaned_text (first 5 rows):\")\n",
    "    print(df['cleaned_text'].head().to_string(index=False))\n",
    "else:\n",
    "    print(\"No suitable text column found to create 'cleaned_text'.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "022e5f63-8142-4e09-8537-c48654a1c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 5: REMOVE STOPWORDS\n",
      "================================================================================\n",
      "\n",
      "Loaded 197 English stopwords (excluding 'not')\n",
      "Sample stopwords: [\"she's\", 'mustn', \"isn't\", \"it's\", 'where', 'did', \"should've\", 'y', 'hadn', 'whom']\n",
      "\n",
      "Applying stopword removal to 'Text'...\n",
      "✓ Created 'text_no_stopwords' column\n",
      "Dataset shape: (5000, 13)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample comparison (Text vs text_no_stopwords):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Row 0:\n",
      "  cleaned_text: I regret choosing this....\n",
      "  text_no_stopwords: regret choosing this....\n",
      "\n",
      "Row 1:\n",
      "  cleaned_text: I regret choosing this....\n",
      "  text_no_stopwords: regret choosing this....\n",
      "\n",
      "Row 2:\n",
      "  cleaned_text: It failed to meet my expectations....\n",
      "  text_no_stopwords: failed meet expectations....\n",
      "\n",
      "Row 3:\n",
      "  cleaned_text: I expected more from this product....\n",
      "  text_no_stopwords: expected product....\n",
      "\n",
      "Row 4:\n",
      "  cleaned_text: This product is amazing!...\n",
      "  text_no_stopwords: product amazing!...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "Non-empty text_no_stopwords: 5000 / 5000\n",
      "Empty text_no_stopwords: 0 / 5000\n",
      "\n",
      "================================================================================\n",
      "✓ Task 5 completed: Stopwords removed successfully (NOT preserved)!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Remove stopwords\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 5: REMOVE STOPWORDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Import nltk stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Download stopwords if not already present\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# >>> IMPORTANT CHANGE: Keep the word \"not\"\n",
    "if \"not\" in stop_words:\n",
    "    stop_words.remove(\"not\")\n",
    "\n",
    "print(f\"\\nLoaded {len(stop_words)} English stopwords (excluding 'not')\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")\n",
    "\n",
    "# Define function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text but keep 'not'.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply to Text column\n",
    "if 'Text' in df.columns:\n",
    "    print(\"\\nApplying stopword removal to 'Text'...\")\n",
    "    df['text_no_stopwords'] = df['Text'].apply(remove_stopwords)\n",
    "    print(f\"✓ Created 'text_no_stopwords' column\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Show comparison\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Sample comparison (Text vs text_no_stopwords):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sample_df = df[['Text', 'text_no_stopwords']].head(5)\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nRow {idx}:\")\n",
    "        print(f\"  cleaned_text: {row['Text'][:80]}...\")\n",
    "        print(f\"  text_no_stopwords: {row['text_no_stopwords'][:80]}...\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    empty_count = (df['text_no_stopwords'] == '').sum()\n",
    "    non_empty_count = len(df) - empty_count\n",
    "    print(f\"Non-empty text_no_stopwords: {non_empty_count} / {len(df)}\")\n",
    "    print(f\"Empty text_no_stopwords: {empty_count} / {len(df)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ 'Text' column not found. Please run Task 4 first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 5 completed: Stopwords removed successfully (NOT preserved)!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20181c77-97ae-4cf4-b8b3-e18b398ff849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 6: EXTRACT HASHTAGS\n",
      "================================================================================\n",
      "Extracting hashtags from column: 'Hashtag'\n",
      "✓ Created 'hashtags' column\n",
      "Dataset shape: (5000, 15)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Hashtag Extraction Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "Total rows: 5000\n",
      "Rows with hashtags: 5000 (100.00%)\n",
      "Rows without hashtags: 0 (0.00%)\n",
      "Total hashtags extracted: 5000\n",
      "Average hashtags per row: 1.00\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample hashtags (first 10 rows with hashtags):\n",
      "--------------------------------------------------------------------------------\n",
      "1. #Challenge\n",
      "2. #Education\n",
      "3. #Challenge\n",
      "4. #Education\n",
      "5. #Dance\n",
      "6. #Challenge\n",
      "7. #Comedy\n",
      "8. #Gaming\n",
      "9. #Education\n",
      "10. #Gaming\n",
      "\n",
      "================================================================================\n",
      "✓ Task 6 completed: Hashtags extracted successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 6: Extract hashtags\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 6: EXTRACT HASHTAGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define function to extract hashtags\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract all hashtags from text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = str(text)\n",
    "    # Match hashtag pattern: # followed by word characters\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    return hashtags\n",
    "\n",
    "# Find the primary text column (raw text, not processed)\n",
    "candidates = [col for col in df.columns \n",
    "              if df[col].dtype == 'object' \n",
    "              and '_normalized' not in col \n",
    "              and '_no_stopwords' not in col \n",
    "              and 'cleaned' not in col]\n",
    "\n",
    "if candidates:\n",
    "    raw_text_col = \"Hashtag\"\n",
    "    print(f\"Extracting hashtags from column: '{raw_text_col}'\")\n",
    "    df['hashtags'] = df[raw_text_col].apply(extract_hashtags)\n",
    "    print(f\"✓ Created 'hashtags' column\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Hashtag Extraction Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    rows_with_hashtags = (df['hashtags'].apply(len) > 0).sum()\n",
    "    rows_without_hashtags = total_rows - rows_with_hashtags\n",
    "    total_hashtags = sum(len(h) for h in df['hashtags'])\n",
    "    avg_hashtags_per_row = total_hashtags / total_rows if total_rows > 0 else 0\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows with hashtags: {rows_with_hashtags} ({rows_with_hashtags/total_rows*100:.2f}%)\")\n",
    "    print(f\"Rows without hashtags: {rows_without_hashtags} ({rows_without_hashtags/total_rows*100:.2f}%)\")\n",
    "    print(f\"Total hashtags extracted: {total_hashtags}\")\n",
    "    print(f\"Average hashtags per row: {avg_hashtags_per_row:.2f}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Sample hashtags (first 10 rows with hashtags):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    rows_with_tags = df[df['hashtags'].apply(len) > 0].head(10)\n",
    "    for idx, (i, row) in enumerate(rows_with_tags.iterrows(), 1):\n",
    "        hashtag_list = ', '.join(row['hashtags'][:5])  # Show max 5 hashtags\n",
    "        if len(row['hashtags']) > 5:\n",
    "            hashtag_list += f\", ... (+{len(row['hashtags']) - 5} more)\"\n",
    "        print(f\"{idx}. {hashtag_list}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No suitable raw text column found to extract hashtags.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 6 completed: Hashtags extracted successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ef0974-1026-4106-8b63-b3f5be9dd7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 7: CALCULATE SENTIMENT SCORES (POLARITY & SUBJECTIVITY)\n",
      "================================================================================\n",
      "Using 'Text' as input for sentiment calculation\n",
      "\n",
      "Added columns: 'polarity' and 'subjectivity' to DataFrame. Dataset shape: (5000, 17)\n",
      "\n",
      "Sample sentiment scores (first 10 rows):\n",
      "                               Text  polarity  subjectivity\n",
      "            I regret choosing this.      0.00          0.00\n",
      "            I regret choosing this.      0.00          0.00\n",
      " It failed to meet my expectations.     -0.50          0.30\n",
      " I expected more from this product.      0.20          0.45\n",
      "           This product is amazing!      0.75          0.90\n",
      " It failed to meet my expectations.     -0.50          0.30\n",
      "              Such a waste of time.     -0.10          0.25\n",
      "Would not recommend this to others.      0.00          0.00\n",
      "            I regret choosing this.      0.00          0.00\n",
      " This could definitely be improved.      0.00          0.50\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sentiment summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Polarity: mean=0.2696, min=-0.7500, max=1.0000\n",
      "Subjectivity: mean=0.5213, min=0.0000, max=1.0000\n",
      "\n",
      "================================================================================\n",
      "✓ Task 7 completed: Sentiment scores calculated.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 7: Calculate sentiment scores\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 7: CALCULATE SENTIMENT SCORES (POLARITY & SUBJECTIVITY)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try to import TextBlob, install if missing\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    print(\"TextBlob not found. Installing textblob...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'textblob'])\n",
    "    from textblob import TextBlob\n",
    "    # Ensure punkt tokenizer available\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except Exception:\n",
    "        import nltk\n",
    "        print('Downloading NLTK punkt tokenizer...')\n",
    "        nltk.download('punkt')\n",
    "\n",
    "# Choose input column: prefer 'text_no_stopwords', then 'cleaned_text', then common raw text names\n",
    "candidates = ['Text', 'cleaned_text','Engagement_Level', 'Platform', 'Hashtag', 'Content_Type', 'Region', 'Views', 'Likes', 'Shares']\n",
    "input_col = 'Text'\n",
    "for c in candidates:\n",
    "    if c in df.columns:\n",
    "        input_col = c\n",
    "        break\n",
    "\n",
    "if input_col is None:\n",
    "    raise ValueError('No suitable text column found for sentiment calculation. Please create `cleaned_text` or `text_no_stopwords` first.')\n",
    "\n",
    "print(f\"Using '{input_col}' as input for sentiment calculation\")\n",
    "\n",
    "# Function to compute polarity & subjectivity\n",
    "def compute_sentiment(text):\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return (None, None)\n",
    "    try:\n",
    "        tb = TextBlob(str(text))\n",
    "        try:\n",
    "            tb = tb.translate(to=\"en\")\n",
    "        except:\n",
    "            pass\n",
    "        return tb.sentiment.polarity, tb.sentiment.subjectivity\n",
    "    except:\n",
    "        return (None, None)\n",
    "\n",
    "# Apply to the dataframe\n",
    "sentiments = df[input_col].fillna('').apply(lambda t: compute_sentiment(t))\n",
    "\n",
    "df['polarity'] = sentiments.apply(lambda x: x[0])\n",
    "df['subjectivity'] = sentiments.apply(lambda x: x[1])\n",
    "\n",
    "# Quick stats and sample\n",
    "print(f\"\\nAdded columns: 'polarity' and 'subjectivity' to DataFrame. Dataset shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nSample sentiment scores (first 10 rows):\")\n",
    "print(df[[input_col, 'polarity', 'subjectivity']].head(10).to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "valid_polarity = df['polarity'].dropna()\n",
    "valid_subjectivity = df['subjectivity'].dropna()\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Sentiment summary:\")\n",
    "print(\"-\" * 80)\n",
    "if len(valid_polarity) > 0:\n",
    "    print(f\"Polarity: mean={valid_polarity.mean():.4f}, min={valid_polarity.min():.4f}, max={valid_polarity.max():.4f}\")\n",
    "else:\n",
    "    print(\"No valid polarity values computed.\")\n",
    "\n",
    "if len(valid_subjectivity) > 0:\n",
    "    print(f\"Subjectivity: mean={valid_subjectivity.mean():.4f}, min={valid_subjectivity.min():.4f}, max={valid_subjectivity.max():.4f}\")\n",
    "else:\n",
    "    print(\"No valid subjectivity values computed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 7 completed: Sentiment scores calculated.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d1ce8b0-a7b2-44c6-aa24-1de63fcb3a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 8: EXPORT CLEANED DATASET\n",
      "================================================================================\n",
      "\n",
      "Exporting cleaned DataFrame to: E:/internship project/data/processed\\clean_data.csv\n",
      "✓ Export successful!\n",
      "  - File path: E:/internship project/data/processed\\clean_data.csv\n",
      "  - File size: 0.79 MB\n",
      "  - Rows: 5000\n",
      "  - Columns: 17\n",
      "\n",
      "  Columns in cleaned dataset:\n",
      "    1. Post_ID\n",
      "    2. Post_Date\n",
      "    3. Platform\n",
      "    4. Hashtag\n",
      "    5. Content_Type\n",
      "    6. Region\n",
      "    7. Views\n",
      "    8. Likes\n",
      "    9. Shares\n",
      "    10. Comments\n",
      "    11. Engagement_Level\n",
      "    12. Text\n",
      "    13. cleaned_text\n",
      "    14. text_no_stopwords\n",
      "    15. hashtags\n",
      "    16. polarity\n",
      "    17. subjectivity\n",
      "\n",
      "================================================================================\n",
      "✓ Task 8 completed: Cleaned dataset exported successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 8: Export cleaned dataset\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 8: EXPORT CLEANED DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'E:/internship project/data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define output file path\n",
    "output_file = os.path.join(output_dir, 'clean_data.csv')\n",
    "\n",
    "# Export to CSV\n",
    "print(f\"\\nExporting cleaned DataFrame to: {output_file}\")\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "# Verify export\n",
    "if os.path.exists(output_file):\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"✓ Export successful!\")\n",
    "    print(f\"  - File path: {output_file}\")\n",
    "    print(f\"  - File size: {file_size:.2f} MB\")\n",
    "    print(f\"  - Rows: {len(df)}\")\n",
    "    print(f\"  - Columns: {len(df.columns)}\")\n",
    "    print(f\"\\n  Columns in cleaned dataset:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"    {i}. {col}\")\n",
    "else:\n",
    "    print(f\"⚠ Export failed. File not found at {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 8 completed: Cleaned dataset exported successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56d50143-1298-4c89-9ab9-c56df31b48d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 9: GENERATE N-GRAMS (UNIGRAMS, BIGRAMS, TRIGRAMS)\n",
      "================================================================================\n",
      "\n",
      "Sentiment distribution:\n",
      "  Positive (polarity > 0.1): 3037\n",
      "  Negative (polarity < -0.1): 985\n",
      "  Neutral: 978\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Generating N-grams for Positive Sentiment:\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 Unigrams (Positive):\n",
      "  experience: 543\n",
      "  could: 275\n",
      "  better.: 275\n",
      "  really: 273\n",
      "  loved: 273\n",
      "  using: 273\n",
      "  this!: 273\n",
      "  Absolutely: 268\n",
      "  worth: 268\n",
      "  it!: 268\n",
      "\n",
      "Top 10 Bigrams (Positive):\n",
      "  experience could: 275\n",
      "  could better.: 275\n",
      "  really loved: 273\n",
      "  loved using: 273\n",
      "  using this!: 273\n",
      "  Absolutely worth: 268\n",
      "  worth it!: 268\n",
      "  Fantastic experience: 268\n",
      "  experience overall.: 268\n",
      "  Better thought.: 260\n",
      "\n",
      "Top 10 Trigrams (Positive):\n",
      "  experience could better.: 275\n",
      "  really loved using: 273\n",
      "  loved using this!: 273\n",
      "  Absolutely worth it!: 268\n",
      "  Fantastic experience overall.: 268\n",
      "  worked perfectly me.: 254\n",
      "  One best things: 248\n",
      "  best things tried.: 248\n",
      "  Great quality performance.: 231\n",
      "  Really happy results.: 230\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Generating N-grams for Negative Sentiment:\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 Unigrams (Negative):\n",
      "  Totally: 258\n",
      "  disappointed: 258\n",
      "  outcome.: 258\n",
      "  failed: 250\n",
      "  meet: 250\n",
      "  expectations.: 250\n",
      "  poor: 246\n",
      "  experience: 246\n",
      "  overall.: 246\n",
      "  satisfied: 231\n",
      "\n",
      "Top 10 Bigrams (Negative):\n",
      "  Totally disappointed: 258\n",
      "  disappointed outcome.: 258\n",
      "  failed meet: 250\n",
      "  meet expectations.: 250\n",
      "  poor experience: 246\n",
      "  experience overall.: 246\n",
      "  satisfied this.: 231\n",
      "\n",
      "Top 10 Trigrams (Negative):\n",
      "  Totally disappointed outcome.: 258\n",
      "  failed meet expectations.: 250\n",
      "  poor experience overall.: 246\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Positive unigrams: 33\n",
      "Positive bigrams: 22\n",
      "Positive trigrams: 10\n",
      "Negative unigrams: 11\n",
      "Negative bigrams: 7\n",
      "Negative trigrams: 3\n",
      "\n",
      "================================================================================\n",
      "✓ Task 9 completed: N-grams generated and analyzed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 9: Generate unigrams/bigrams/trigrams\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 9: GENERATE N-GRAMS (UNIGRAMS, BIGRAMS, TRIGRAMS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Define sentiment thresholds\n",
    "# Positive: polarity > 0.1, Negative: polarity < -0.1, Neutral: in between\n",
    "positive_threshold = 0.1\n",
    "negative_threshold = -0.1\n",
    "\n",
    "# Categorize rows by sentiment\n",
    "df['sentiment_category'] = df['polarity'].apply(\n",
    "    lambda x: 'positive' if x > positive_threshold \n",
    "              else ('negative' if x < negative_threshold else 'neutral')\n",
    ")\n",
    "\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(f\"  Positive (polarity > {positive_threshold}): {(df['sentiment_category'] == 'positive').sum()}\")\n",
    "print(f\"  Negative (polarity < {negative_threshold}): {(df['sentiment_category'] == 'negative').sum()}\")\n",
    "print(f\"  Neutral: {(df['sentiment_category'] == 'neutral').sum()}\")\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"Generate n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return []\n",
    "    words = str(text).split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Generate n-grams for positive and negative sentiments\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Generating N-grams for Positive Sentiment:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "positive_texts = df[df['sentiment_category'] == 'positive']['text_no_stopwords']\n",
    "positive_unigrams = Counter()\n",
    "positive_bigrams = Counter()\n",
    "positive_trigrams = Counter()\n",
    "\n",
    "for text in positive_texts:\n",
    "    positive_unigrams.update(generate_ngrams(text, 1))\n",
    "    positive_bigrams.update(generate_ngrams(text, 2))\n",
    "    positive_trigrams.update(generate_ngrams(text, 3))\n",
    "\n",
    "print(f\"Top 10 Unigrams (Positive):\")\n",
    "for gram, freq in positive_unigrams.most_common(10):\n",
    "    print(f\"  {gram[0]}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Bigrams (Positive):\")\n",
    "for gram, freq in positive_bigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Trigrams (Positive):\")\n",
    "for gram, freq in positive_trigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "# Generate n-grams for negative sentiment\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Generating N-grams for Negative Sentiment:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "negative_texts = df[df['sentiment_category'] == 'negative']['text_no_stopwords']\n",
    "negative_unigrams = Counter()\n",
    "negative_bigrams = Counter()\n",
    "negative_trigrams = Counter()\n",
    "\n",
    "for text in negative_texts:\n",
    "    negative_unigrams.update(generate_ngrams(text, 1))\n",
    "    negative_bigrams.update(generate_ngrams(text, 2))\n",
    "    negative_trigrams.update(generate_ngrams(text, 3))\n",
    "\n",
    "print(f\"Top 10 Unigrams (Negative):\")\n",
    "for gram, freq in negative_unigrams.most_common(10):\n",
    "    print(f\"  {gram[0]}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Bigrams (Negative):\")\n",
    "for gram, freq in negative_bigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "print(f\"\\nTop 10 Trigrams (Negative):\")\n",
    "for gram, freq in negative_trigrams.most_common(10):\n",
    "    print(f\"  {' '.join(gram)}: {freq}\")\n",
    "\n",
    "# Create frequency DataFrames for export\n",
    "pos_unigram_df = pd.DataFrame(\n",
    "    [(gram[0], freq) for gram, freq in positive_unigrams.most_common(50)],\n",
    "    columns=['unigram', 'frequency']\n",
    ")\n",
    "pos_bigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in positive_bigrams.most_common(50)],\n",
    "    columns=['bigram', 'frequency']\n",
    ")\n",
    "pos_trigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in positive_trigrams.most_common(50)],\n",
    "    columns=['trigram', 'frequency']\n",
    ")\n",
    "\n",
    "neg_unigram_df = pd.DataFrame(\n",
    "    [(gram[0], freq) for gram, freq in negative_unigrams.most_common(50)],\n",
    "    columns=['unigram', 'frequency']\n",
    ")\n",
    "neg_bigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in negative_bigrams.most_common(50)],\n",
    "    columns=['bigram', 'frequency']\n",
    ")\n",
    "neg_trigram_df = pd.DataFrame(\n",
    "    [(' '.join(gram), freq) for gram, freq in negative_trigrams.most_common(50)],\n",
    "    columns=['trigram', 'frequency']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Positive unigrams: {len(positive_unigrams)}\")\n",
    "print(f\"Positive bigrams: {len(positive_bigrams)}\")\n",
    "print(f\"Positive trigrams: {len(positive_trigrams)}\")\n",
    "print(f\"Negative unigrams: {len(negative_unigrams)}\")\n",
    "print(f\"Negative bigrams: {len(negative_bigrams)}\")\n",
    "print(f\"Negative trigrams: {len(negative_trigrams)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 9 completed: N-grams generated and analyzed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc566ea0-51c9-4282-8d3e-5630b74e5a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 10: TF-IDF ANALYSIS (OVERALL TEXT)\n",
      "================================================================================\n",
      "Using 'cleaned_text' column\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Top TF-IDF Keywords (Overall Dataset):\n",
      "--------------------------------------------------------------------------------\n",
      "fitness : 0.1072\n",
      "education : 0.1050\n",
      "challenge : 0.1014\n",
      "comedy : 0.1010\n",
      "dance : 0.0992\n",
      "music : 0.0986\n",
      "tech : 0.0982\n",
      "fashion : 0.0974\n",
      "viral : 0.0962\n",
      "gaming : 0.0958\n",
      "\n",
      "================================================================================\n",
      "✓ Task 10 completed successfully (overall TF-IDF).\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 10: TF-IDF Analysis (Overall Corpus)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 10: TF-IDF ANALYSIS (OVERALL TEXT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Detect text column\n",
    "text_col = None\n",
    "candidates = ['cleaned_text', 'text', 'tweet', 'content', 'post', 'message', 'caption', 'body']\n",
    "for col in candidates:\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    raise ValueError(\"No suitable text column found.\")\n",
    "\n",
    "print(f\"Using '{text_col}' column\")\n",
    "\n",
    "# Prepare text data\n",
    "texts = df[text_col].dropna().astype(str).tolist()\n",
    "\n",
    "if len(texts) == 0:\n",
    "    raise ValueError(\"Text column is empty.\")\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(texts)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Mean TF-IDF\n",
    "mean_tfidf = np.asarray(X.mean(axis=0)).flatten()\n",
    "\n",
    "# Top keywords\n",
    "top_n = 20\n",
    "top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top TF-IDF Keywords (Overall Dataset):\")\n",
    "print(\"-\" * 80)\n",
    "for i in top_idx:\n",
    "    print(f\"{feature_names[i]} : {mean_tfidf[i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 10 completed successfully (overall TF-IDF).\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08db1373-57c4-4b3d-94c6-cb3fb78a0e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 11: EMOJI EXTRACTION + SENTIMENT MAPPING\n",
      "================================================================================\n",
      "Using 'Post_ID' column for emoji extraction\n",
      "\n",
      "Extracting emojis from text...\n",
      "\n",
      "⚠ No emojis detected — emoji sentiment analysis skipped.\n",
      "Total unique emojis found: 0\n",
      "Total emoji occurrences: 0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Top 20 Emojis by Frequency + Sentiment Score:\n",
      "--------------------------------------------------------------------------------\n",
      "No emojis found in the dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Emoji Sentiment Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "No emojis found.\n",
      "\n",
      "================================================================================\n",
      "✓ Task 11 completed: Emojis extracted and sentiment mapped!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 11: Emoji extraction + sentiment mapping\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 11: EMOJI EXTRACTION + SENTIMENT MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "# Try to import emoji library, install if missing\n",
    "try:\n",
    "    import emoji\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    print(\"emoji library not found. Installing emoji...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'emoji'])\n",
    "    import emoji\n",
    "\n",
    "# Find raw text column (original unprocessed text with emojis)\n",
    "raw_text_col = None\n",
    "candidates = [col for col in df.columns \n",
    "              if df[col].dtype == 'object' \n",
    "              and 'cleaned' not in col \n",
    "              and '_no_stopwords' not in col \n",
    "              and 'hashtags' not in col]\n",
    "\n",
    "if candidates:\n",
    "    raw_text_col = candidates[0]\n",
    "    print(f\"Using '{raw_text_col}' column for emoji extraction\")\n",
    "else:\n",
    "    raise ValueError('No suitable raw text column found for emoji extraction.')\n",
    "\n",
    "# Function to extract emojis from text\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract all emojis from text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = str(text)\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "# Extract emojis\n",
    "print(\"\\nExtracting emojis from text...\")\n",
    "df['emojis'] = df[raw_text_col].apply(extract_emojis)\n",
    "\n",
    "# Build sentiment mapping\n",
    "emoji_sentiment_map = {}\n",
    "emoji_counts = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    emojis_in_text = row['emojis']\n",
    "    polarity = row['polarity']\n",
    "    \n",
    "    for em in emojis_in_text:\n",
    "        if em not in emoji_sentiment_map:\n",
    "            emoji_sentiment_map[em] = []\n",
    "            emoji_counts[em] = 0\n",
    "        emoji_sentiment_map[em].append(polarity)\n",
    "        emoji_counts[em] += 1\n",
    "\n",
    "# Build summary list\n",
    "emoji_sentiment_summary = []\n",
    "for em, sentiments in emoji_sentiment_map.items():\n",
    "    avg_sentiment = np.mean(sentiments)\n",
    "    std_sentiment = np.std(sentiments)\n",
    "    count = len(sentiments)\n",
    "\n",
    "    try:\n",
    "        emoji_desc = emoji.demojize(em)\n",
    "    except:\n",
    "        emoji_desc = \"unknown\"\n",
    "\n",
    "    emoji_sentiment_summary.append({\n",
    "        'emoji': em,\n",
    "        'description': emoji_desc,\n",
    "        'avg_polarity': avg_sentiment,\n",
    "        'std_polarity': std_sentiment,\n",
    "        'count': count\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame with safe checks\n",
    "emoji_df = pd.DataFrame(emoji_sentiment_summary)\n",
    "\n",
    "# FIX: handle empty dataframe safely\n",
    "if emoji_df.empty or 'count' not in emoji_df.columns:\n",
    "    print(\"\\n⚠ No emojis detected — emoji sentiment analysis skipped.\")\n",
    "    print(\"Total unique emojis found: 0\")\n",
    "    print(\"Total emoji occurrences: 0\")\n",
    "    emoji_df = pd.DataFrame()   # keep empty\n",
    "else:\n",
    "    emoji_df = emoji_df.sort_values('count', ascending=False)\n",
    "    print(f\"\\nTotal unique emojis found: {len(emoji_df)}\")\n",
    "    print(f\"Total emoji occurrences: {emoji_df['count'].sum()}\")\n",
    "\n",
    "# Display top emojis\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top 20 Emojis by Frequency + Sentiment Score:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if not emoji_df.empty:\n",
    "    display_df = emoji_df.head(20)[['emoji', 'description', 'avg_polarity', 'count']]\n",
    "    for idx, row in display_df.iterrows():\n",
    "        emoji_char = row['emoji']\n",
    "        desc = row['description']\n",
    "        avg_pol = row['avg_polarity']\n",
    "        count = row['count']\n",
    "        sentiment_label = (\n",
    "            \"Positive\" if avg_pol > 0.1 \n",
    "            else (\"Negative\" if avg_pol < -0.1 else \"Neutral\")\n",
    "        )\n",
    "        print(f\"{emoji_char} {desc}: avg_polarity={avg_pol:.4f} ({sentiment_label}), count={count}\")\n",
    "else:\n",
    "    print(\"No emojis found in the dataset.\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Emoji Sentiment Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if not emoji_df.empty:\n",
    "    print(f\"Average polarity of emojis: {emoji_df['avg_polarity'].mean():.4f}\")\n",
    "    print(f\"Most positive emoji avg_polarity: {emoji_df['avg_polarity'].max():.4f}\")\n",
    "    print(f\"Most negative emoji avg_polarity: {emoji_df['avg_polarity'].min():.4f}\")\n",
    "\n",
    "    positive_emojis = len(emoji_df[emoji_df['avg_polarity'] > 0.1])\n",
    "    negative_emojis = len(emoji_df[emoji_df['avg_polarity'] < -0.1])\n",
    "    neutral_emojis = len(emoji_df[(emoji_df['avg_polarity'] >= -0.1) & (emoji_df['avg_polarity'] <= 0.1)])\n",
    "\n",
    "    print(f\"Positive emojis: {positive_emojis}\")\n",
    "    print(f\"Negative emojis: {negative_emojis}\")\n",
    "    print(f\"Neutral emojis: {neutral_emojis}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 11 completed: Emojis extracted and sentiment mapped!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82656ab1-13f3-40be-a63b-ae28ed2445c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 12: HASHTAG FREQUENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total unique hashtags: 10\n",
      "Total hashtag occurrences: 5000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Top 30 Hashtags by Frequency:\n",
      "--------------------------------------------------------------------------------\n",
      "#Fitness: 536 (10.72%)\n",
      "#Education: 525 (10.5%)\n",
      "#Challenge: 507 (10.14%)\n",
      "#Comedy: 505 (10.1%)\n",
      "#Dance: 496 (9.92%)\n",
      "#Music: 493 (9.86%)\n",
      "#Tech: 491 (9.82%)\n",
      "#Fashion: 487 (9.74%)\n",
      "#Viral: 481 (9.62%)\n",
      "#Gaming: 479 (9.58%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Hashtag Frequency Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "Most frequent hashtag: #Fitness (536 occurrences)\n",
      "Average hashtag frequency: 500.00\n",
      "Median hashtag frequency: 494.50\n",
      "Max frequency: 536\n",
      "Min frequency: 479\n",
      "\n",
      "Hashtags with 2+ occurrences (trending): 10\n",
      "Hashtags appearing only once: 0\n",
      "\n",
      "Top 10 hashtags contribute: 100.00% of all hashtag occurrences\n",
      "\n",
      "================================================================================\n",
      "✓ Task 12 completed: Hashtag frequency analysis completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 12: Hashtag frequency analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 12: HASHTAG FREQUENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Extract and flatten all hashtags from the hastags column \n",
    "all_hashtags = []\n",
    "for hashtag_list in df['hashtags']:\n",
    "    if isinstance(hashtag_list, list):\n",
    "        all_hashtags.extend(hashtag_list)\n",
    "\n",
    "# count hastags frequencies\n",
    "hashtag_counter = Counter(all_hashtags)\n",
    "\n",
    "# create summary dataframe \n",
    "hashtag_freq_df = pd.DataFrame(\n",
    "    list(hashtag_counter.most_common()),\n",
    "    columns=['hashtag', 'count']\n",
    ")\n",
    "\n",
    "# add percentage column \n",
    "total_hashtags = hashtag_freq_df['count'].sum()\n",
    "hashtag_freq_df['percentage'] = (hashtag_freq_df['count'] / total_hashtags * 100).round(2)\n",
    "\n",
    "print(f\"\\nTotal unique hashtags: {len(hashtag_freq_df)}\")\n",
    "print(f\"Total hashtag occurrences: {total_hashtags}\")\n",
    "\n",
    "# Display top 30 hashtags\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top 30 Hashtags by Frequency:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_30 = hashtag_freq_df.head(30)\n",
    "for idx, row in top_30.iterrows():\n",
    "    hashtag = row['hashtag']\n",
    "    count = row['count']\n",
    "    percentage = row['percentage']\n",
    "    print(f\"{hashtag}: {count} ({percentage}%)\")\n",
    "\n",
    "# statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Hashtag Frequency Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Most frequent hashtag: {hashtag_freq_df.iloc[0]['hashtag']} ({hashtag_freq_df.iloc[0]['count']} occurrences)\")\n",
    "print(f\"Average hashtag frequency: {hashtag_freq_df['count'].mean():.2f}\")\n",
    "print(f\"Median hashtag frequency: {hashtag_freq_df['count'].median():.2f}\")\n",
    "print(f\"Max frequency: {hashtag_freq_df['count'].max()}\")\n",
    "print(f\"Min frequency: {hashtag_freq_df['count'].min()}\")\n",
    "\n",
    "# Trending analysis: hashtags appearing more than once\n",
    "trending_threshold = 2\n",
    "trending_hashtags = hashtag_freq_df[hashtag_freq_df['count'] >= trending_threshold]\n",
    "print(f\"\\nHashtags with 2+ occurrences (trending): {len(trending_hashtags)}\")\n",
    "print(f\"Hashtags appearing only once: {len(hashtag_freq_df[hashtag_freq_df['count'] == 1])}\")\n",
    "\n",
    "# top hastags contributions   \n",
    "top_10_contribution = top_30['count'].sum() / total_hashtags * 100\n",
    "print(f\"\\nTop 10 hashtags contribute: {top_10_contribution:.2f}% of all hashtag occurrences\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 12 completed: Hashtag frequency analysis completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f44caa93-f968-4224-8c48-6f93f943ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 13: HASHTAG SENTIMENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Saved hashtag sentiment table to:\n",
      "C:\\Users\\dell\\data\\processed\\hashtag_sentiment.csv\n",
      "Total unique hashtags: 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Top 30 hashtags with sentiment:\n",
      "--------------------------------------------------------------------------------\n",
      "hashtag_norm  avg_polarity  std_polarity  count  percentage\n",
      "    #fitness      0.296679      0.505280    536       10.72\n",
      "  #education      0.270190      0.514876    525       10.50\n",
      "  #challenge      0.230424      0.503382    507       10.14\n",
      "     #comedy      0.280634      0.471777    505       10.10\n",
      "      #dance      0.275081      0.491044    496        9.92\n",
      "      #music      0.255832      0.497994    493        9.86\n",
      "       #tech      0.255621      0.490553    491        9.82\n",
      "    #fashion      0.284569      0.474373    487        9.74\n",
      "      #viral      0.300644      0.490059    481        9.62\n",
      "     #gaming      0.244843      0.501876    479        9.58\n",
      "\n",
      "================================================================================\n",
      "✓ Task 13 completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Task 13: Hashtag Sentiment Analysis\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 13: HASHTAG SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Safety checks\n",
    "# -----------------------------\n",
    "if 'hashtags' not in df.columns:\n",
    "    raise ValueError(\"Column 'hashtags' not found. Run Task 6 first.\")\n",
    "if 'polarity' not in df.columns:\n",
    "    raise ValueError(\"Column 'polarity' not found. Run Task 7 first.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Explode hashtags\n",
    "# -----------------------------\n",
    "expanded = df[['hashtags', 'polarity']].explode('hashtags')\n",
    "expanded = expanded.dropna(subset=['hashtags'])\n",
    "\n",
    "# Normalize hashtags\n",
    "expanded['hashtag_norm'] = expanded['hashtags'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate sentiment per hastag\n",
    "# -----------------------------\n",
    "hashtag_sentiment = (\n",
    "    expanded\n",
    "    .groupby('hashtag_norm')\n",
    "    .agg(\n",
    "        avg_polarity=('polarity', 'mean'),\n",
    "        std_polarity=('polarity', 'std'),\n",
    "        count=('polarity', 'size')   # size() is safer than count()\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Percentage calculation\n",
    "# -----------------------------\n",
    "total_hashtags = hashtag_sentiment['count'].sum()\n",
    "hashtag_sentiment['percentage'] = (\n",
    "    hashtag_sentiment['count'] / total_hashtags * 100\n",
    ").round(2)\n",
    "\n",
    "# -----------------------------\n",
    "# Sort by frequency\n",
    "# -----------------------------\n",
    "hashtag_sentiment = hashtag_sentiment.sort_values(\n",
    "    by='count',\n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Save output (safe path)\n",
    "# -----------------------------\n",
    "out_dir = os.path.join(os.getcwd(), 'data', 'processed')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "out_file = os.path.join(out_dir, 'hashtag_sentiment.csv')\n",
    "hashtag_sentiment.to_csv(out_file, index=False, encoding='utf-8')\n",
    "\n",
    "# -----------------------------\n",
    "# Display results\n",
    "# -----------------------------\n",
    "print(f\"\\nSaved hashtag sentiment table to:\\n{out_file}\")\n",
    "print(f\"Total unique hashtags: {len(hashtag_sentiment)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Top 30 hashtags with sentiment:\")\n",
    "print(\"-\" * 80)\n",
    "print(hashtag_sentiment.head(30).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Task 13 completed successfully!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f02cfacd-25a5-4e9b-8bc8-eaf72a7399b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ sentiment_summary.csv created\n",
      "✓ hashtag_sentiment.csv created\n",
      "✓ keyword_drivers.csv created\n",
      "✓ emoji_sentiment.csv created\n",
      "\n",
      "🎉 All Power BI Summary Tables Generated Successfully!\n",
      "📁 Files Created:\n",
      " - sentiment_summary.csv\n",
      " - hashtag_sentiment.csv\n",
      " - keyword_drivers.csv\n",
      " - emoji_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TASK 14 - CREATE POWER BI TABLES\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --------------------\n",
    "# Set Output Directory\n",
    "# --------------------\n",
    "OUT_DIR = r\"E:/internship project/data/processed\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------\n",
    "# df must already exist\n",
    "# Required columns:\n",
    "# polarity\n",
    "# hashtags  (list)\n",
    "# emoji     (list)\n",
    "# *_no_stopwords text\n",
    "# --------------------\n",
    "\n",
    "# ================================\n",
    "# 1️⃣ SENTIMENT SUMMARY TABLE\n",
    "# ================================\n",
    "polarity_cols = [c for c in df.columns if \"polarity\" in c.lower()]\n",
    "if not polarity_cols:\n",
    "    raise ValueError(\"❌ No polarity column found\")\n",
    "\n",
    "polarity_col = polarity_cols[0]\n",
    "\n",
    "sentiment_summary = pd.DataFrame({\n",
    "    \"sentiment\": [\"Positive\", \"Negative\", \"Neutral\"],\n",
    "    \"count\": [\n",
    "        (df[polarity_col] > 0.1).sum(),\n",
    "        (df[polarity_col] < -0.1).sum(),\n",
    "        ((df[polarity_col] >= -0.1) & (df[polarity_col] <= 0.1)).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "sentiment_summary[\"percentage\"] = (\n",
    "    sentiment_summary[\"count\"] / sentiment_summary[\"count\"].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "sentiment_summary.to_csv(os.path.join(OUT_DIR, \"sentiment_summary.csv\"), index=False)\n",
    "print(\"✓ sentiment_summary.csv created\")\n",
    "\n",
    "# ================================\n",
    "# 2️⃣ HASHTAG SENTIMENT TABLE\n",
    "# ================================\n",
    "hashtag_cols = [c for c in df.columns if \"hashtag\" in c.lower()]\n",
    "if not hashtag_cols:\n",
    "    raise ValueError(\"❌ No hashtag column found\")\n",
    "\n",
    "hashtag_col = hashtag_cols[0]\n",
    "\n",
    "expanded = df[[hashtag_col, polarity_col]].explode(hashtag_col).dropna(subset=[hashtag_col])\n",
    "expanded[\"hashtag\"] = expanded[hashtag_col].astype(str).str.lower()\n",
    "\n",
    "hashtag_sentiment = (\n",
    "    expanded.groupby(\"hashtag\")\n",
    "    .agg(\n",
    "        avg_polarity=(polarity_col, \"mean\"),\n",
    "        std_polarity=(polarity_col, \"std\"),\n",
    "        count=(polarity_col, \"size\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "total = hashtag_sentiment[\"count\"].sum()\n",
    "hashtag_sentiment[\"percentage\"] = (hashtag_sentiment[\"count\"] / total * 100).round(2)\n",
    "\n",
    "hashtag_sentiment = hashtag_sentiment.sort_values(\"count\", ascending=False)\n",
    "\n",
    "hashtag_sentiment.to_csv(os.path.join(OUT_DIR, \"hashtag_sentiment.csv\"), index=False)\n",
    "print(\"✓ hashtag_sentiment.csv created\")\n",
    "\n",
    "# ================================\n",
    "# 3️⃣ KEYWORD DRIVERS (TF-IDF)\n",
    "# ================================\n",
    "text_cols = [c for c in df.columns if \"no_stopwords\" in c.lower()]\n",
    "if not text_cols:\n",
    "    raise ValueError(\"❌ No cleaned text column (*_no_stopwords) found\")\n",
    "\n",
    "text_col = text_cols[0]\n",
    "\n",
    "pos_texts = df[df[polarity_col] > 0.1][text_col].dropna()\n",
    "neg_texts = df[df[polarity_col] < -0.1][text_col].dropna()\n",
    "\n",
    "def extract_tfidf(texts, sentiment, top_n=30):\n",
    "    if texts.empty:\n",
    "        return pd.DataFrame()\n",
    "    vec = TfidfVectorizer(max_features=top_n)\n",
    "    X = vec.fit_transform(texts)\n",
    "    return pd.DataFrame({\n",
    "        \"keyword\": vec.get_feature_names_out(),\n",
    "        \"tfidf_score\": X.mean(axis=0).A1,\n",
    "        \"sentiment\": sentiment\n",
    "    })\n",
    "\n",
    "keyword_drivers = pd.concat([\n",
    "    extract_tfidf(pos_texts, \"Positive\"),\n",
    "    extract_tfidf(neg_texts, \"Negative\")\n",
    "], ignore_index=True)\n",
    "\n",
    "keyword_drivers.to_csv(os.path.join(OUT_DIR, \"keyword_drivers.csv\"), index=False)\n",
    "print(\"✓ keyword_drivers.csv created\")\n",
    "\n",
    "# ================================\n",
    "# 4️⃣ EMOJI SENTIMENT TABLE\n",
    "# ================================\n",
    "emoji_cols = [c for c in df.columns if \"emoji\" in c.lower()]\n",
    "if not emoji_cols:\n",
    "    raise ValueError(\"❌ No emoji column found\")\n",
    "\n",
    "emoji_col = emoji_cols[0]\n",
    "\n",
    "emoji_expanded = df[[emoji_col, polarity_col]].explode(emoji_col).dropna(subset=[emoji_col])\n",
    "\n",
    "emoji_sentiment = (\n",
    "    emoji_expanded.groupby(emoji_col)\n",
    "    .agg(\n",
    "        avg_polarity=(polarity_col, \"mean\"),\n",
    "        count=(polarity_col, \"size\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={emoji_col: \"emoji\"})\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "emoji_sentiment.to_csv(os.path.join(OUT_DIR, \"emoji_sentiment.csv\"), index=False)\n",
    "print(\"✓ emoji_sentiment.csv created\")\n",
    "\n",
    "# ================================\n",
    "# DONE\n",
    "# ================================\n",
    "print(\"\\n🎉 All Power BI Summary Tables Generated Successfully!\")\n",
    "print(\"📁 Files Created:\")\n",
    "print(\" - sentiment_summary.csv\")\n",
    "print(\" - hashtag_sentiment.csv\")\n",
    "print(\" - keyword_drivers.csv\")\n",
    "print(\" - emoji_sentiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c780e774-9992-4331-a11a-d437b3d15ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 15: FILTER NEGATIVE SENTIMENT RECORDS -> negative_df\n",
      "================================================================================\n",
      "Negative records: 985 rows\n",
      "\n",
      "Sample negative rows (first 5):\n",
      "Post_ID  Post_Date  Platform    Hashtag Content_Type    Region   Views  Likes  Shares  Comments Engagement_Level                                   Text cleaned_text             text_no_stopwords     hashtags  polarity  subjectivity sentiment_category emojis\n",
      " Post_3 2022-01-07   Twitter #Challenge        Video    Brazil 3666211 327143   39423     36223           Medium     It failed to meet my expectations.    challenge     failed meet expectations. [#Challenge]     -0.50          0.30           negative     []\n",
      " Post_6 2022-11-23 Instagram #Challenge       Shorts Australia 1323566 136282   86979     47129              Low     It failed to meet my expectations.    challenge     failed meet expectations. [#Challenge]     -0.50          0.30           negative     []\n",
      "Post_13 2022-03-16   YouTube     #Viral       Shorts    Canada 4105651 195560   37627     49089             High Totally disappointed with the outcome.        viral Totally disappointed outcome.     [#Viral]     -0.75          0.75           negative     []\n",
      "Post_20 2023-04-24   YouTube #Education        Tweet        UK 3143932 298685   84720     37162              Low Totally disappointed with the outcome.    education Totally disappointed outcome. [#Education]     -0.75          0.75           negative     []\n",
      "Post_21 2023-08-15   Twitter    #Gaming        Video    Brazil   14095 268310   90495     16968              Low     It failed to meet my expectations.       gaming     failed meet expectations.    [#Gaming]     -0.50          0.30           negative     []\n",
      "\n",
      "Saved negative_df to: E:/internship project/data/processed\\negative_df.csv\n",
      "\n",
      "================================================================================\n",
      "✓ Task 15 completed: `negative_df` created and exported.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 15: Filter only negative sentiment records\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 15: FILTER NEGATIVE SENTIMENT RECORDS -> negative_df')\n",
    "print('=' * 80)\n",
    "\n",
    "import os\n",
    "\n",
    "# Ensure polarity column exists\n",
    "if 'polarity' not in df.columns:\n",
    "    raise ValueError(\"Column 'polarity' not found. Run Task 7 to compute sentiment polarity before Task 15.\")\n",
    "\n",
    "# Ensure sentiment_category exists; compute if missing using same thresholds as Task 9\n",
    "positive_threshold = 0.1\n",
    "negative_threshold = -0.1\n",
    "if 'sentiment_category' not in df.columns:\n",
    "    print(\"'sentiment_category' not found — computing from 'polarity' using thresholds.\")\n",
    "    df['sentiment_category'] = df['polarity'].apply(\n",
    "        lambda x: 'positive' if x > positive_threshold else ('negative' if x < negative_threshold else 'neutral')\n",
    "    )\n",
    "\n",
    "# Filter negatives\n",
    "negative_df = df[df['sentiment_category'] == 'negative'].copy()\n",
    "\n",
    "print(f\"Negative records: {len(negative_df)} rows\")\n",
    "if len(negative_df) > 0:\n",
    "    print('\\nSample negative rows (first 5):')\n",
    "    print(negative_df.head(5).to_string(index=False))\n",
    "else:\n",
    "    print('No negative records found.')\n",
    "\n",
    "# Optionally save to CSV for downstream use\n",
    "out_dir = r\"E:/internship project/data/processed\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "neg_path = os.path.join(out_dir, 'negative_df.csv')\n",
    "negative_df.to_csv(neg_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nSaved negative_df to: {neg_path}\")\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 15 completed: `negative_df` created and exported.')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7db2e460-1176-4623-8d4b-302278edf579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 16: NEGATIVE BIGRAM & TRIGRAM FREQUENCY\n",
      "================================================================================\n",
      "Negative rows to process: 985\n",
      "Using `text_no_stopwords` for n-gram extraction\n",
      "Top 30 Negative Bigrams:\n",
      "  Totally disappointed: 258\n",
      "  disappointed outcome.: 258\n",
      "  failed meet: 250\n",
      "  meet expectations.: 250\n",
      "  poor experience: 246\n",
      "  experience overall.: 246\n",
      "  satisfied this.: 231\n",
      "Top 30 Negative Trigrams:\n",
      "  Totally disappointed outcome.: 258\n",
      "  failed meet expectations.: 250\n",
      "  poor experience overall.: 246\n",
      "\n",
      "Saved negative bigrams to: E:/internship project/data/processed\\negative_bigrams.csv\n",
      "Saved negative trigrams to: E:/internship project/data/processed\\negative_trigrams.csv\n",
      "\n",
      "================================================================================\n",
      "✓ Task 16 completed: negative n-gram frequency tables created and exported.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 16: Extract negative n-grams (bigrams & trigrams)\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 16: NEGATIVE BIGRAM & TRIGRAM FREQUENCY')\n",
    "print('=' * 80)\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we have a negative dataframe (negative_df) or compute it\n",
    "if 'negative_df' not in globals():\n",
    "    print('`negative_df` not found in memory — computing from `df`.')\n",
    "    if 'polarity' not in df.columns:\n",
    "        raise ValueError(\"Column 'polarity' not found. Run sentiment Task 7 first.\")\n",
    "    # use same thresholds as Task 9\n",
    "    pos_thr = 0.1\n",
    "    neg_thr = -0.1\n",
    "    if 'sentiment_category' not in df.columns:\n",
    "        df['sentiment_category'] = df['polarity'].apply(lambda x: 'positive' if x>pos_thr else ('negative' if x<neg_thr else 'neutral'))\n",
    "    negative_df = df[df['sentiment_category'] == 'negative'].copy()\n",
    "\n",
    "print(f'Negative rows to process: {len(negative_df)}')\n",
    "\n",
    "# Pick text column preference (prefer tokenized/no-stopwords)\n",
    "candidates = ['text_no_stopwords','cleaned_text','text','tweet','content','post','message','caption','body']\n",
    "text_col = None\n",
    "for c in candidates:\n",
    "    if c in negative_df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    print('⚠ No suitable text column found in negative_df. Skipping n-gram extraction.')\n",
    "else:\n",
    "    print(f'Using `{text_col}` for n-gram extraction')\n",
    "    texts = negative_df[text_col].fillna('').astype(str).tolist()\n",
    "    bigram_counter = Counter()\n",
    "    trigram_counter = Counter()\n",
    "\n",
    "    for t in texts:\n",
    "        tokens = t.split()\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        bigrams = ngrams(tokens, 2)\n",
    "        trigrams = ngrams(tokens, 3) if len(tokens) >= 3 else []\n",
    "        bigram_counter.update([' '.join(g) for g in bigrams])\n",
    "        trigram_counter.update([' '.join(g) for g in trigrams])\n",
    "\n",
    "    # Create DataFrames and export top N\n",
    "    top_n = 200\n",
    "    neg_bigram_df = pd.DataFrame(bigram_counter.most_common(top_n), columns=['bigram','frequency'])\n",
    "    neg_trigram_df = pd.DataFrame(trigram_counter.most_common(top_n), columns=['trigram','frequency'])\n",
    "\n",
    "    out_dir = r\"E:/internship project/data/processed\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    bigram_path = os.path.join(out_dir, 'negative_bigrams.csv')\n",
    "    trigram_path = os.path.join(out_dir, 'negative_trigrams.csv')\n",
    "    neg_bigram_df.to_csv(bigram_path, index=False, encoding='utf-8')\n",
    "    neg_trigram_df.to_csv(trigram_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print('Top 30 Negative Bigrams:')\n",
    "    for gram, freq in neg_bigram_df.head(30).values:\n",
    "        print(f'  {gram}: {freq}')\n",
    "\n",
    "    print('Top 30 Negative Trigrams:')\n",
    "    for gram, freq in neg_trigram_df.head(30).values:\n",
    "        print(f'  {gram}: {freq}')\n",
    "\n",
    "    print(f'\\nSaved negative bigrams to: {bigram_path}')\n",
    "    print(f'Saved negative trigrams to: {trigram_path}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 16 completed: negative n-gram frequency tables created and exported.')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af5ab45d-a44a-432e-9bf3-77edc817e897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 17: TOPIC MODELING ON NEGATIVE POSTS\n",
      "================================================================================\n",
      "Negative documents: 10\n",
      "\n",
      "Top Topics & Keywords:\n",
      "     topic                                           keywords\n",
      "0  topic_0  experience poor overall improved definitely sa...\n",
      "1  topic_1  outcome disappointed totally meet expectations...\n",
      "2  topic_2  choosing regret waste time product expected be...\n",
      "\n",
      "Saved negative_df to: E:/internship project/data/processed\\LDA2.csv\n",
      "\n",
      "Negative Posts with Topic Labels:\n",
      "                                     Text topic_label\n",
      "0      I expected more from this product.     topic_2\n",
      "1                 I regret choosing this.     topic_2\n",
      "2      It failed to meet my expectations.     topic_1\n",
      "3         Not satisfied at all with this.     topic_0\n",
      "4                   Such a waste of time.     topic_2\n",
      "5  The experience could have been better.     topic_2\n",
      "6      This could definitely be improved.     topic_0\n",
      "7  Totally disappointed with the outcome.     topic_1\n",
      "8           Very poor experience overall.     topic_0\n",
      "9     Would not recommend this to others.     topic_1\n",
      "\n",
      "================================================================================\n",
      "✓ Task 17 Completed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 17: TOPIC MODELING LDA\n",
    "# ============================\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 17: TOPIC MODELING ON NEGATIVE POSTS')\n",
    "print('=' * 80)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# ---------------------------\n",
    "# NEGATIVE TEXT INPUT\n",
    "# ---------------------------\n",
    "negative_texts = [\n",
    "\"I expected more from this product.\",\n",
    "\"I regret choosing this.\",\n",
    "\"It failed to meet my expectations.\",\n",
    "\"Not satisfied at all with this.\",\n",
    "\"Such a waste of time.\",\n",
    "\"The experience could have been better.\",\n",
    "\"This could definitely be improved.\",\n",
    "\"Totally disappointed with the outcome.\",\n",
    "\"Very poor experience overall.\",\n",
    "\"Would not recommend this to others.\"\n",
    "]\n",
    "\n",
    "negative_df = pd.DataFrame({\"Text\": negative_texts})\n",
    "print(f\"Negative documents: {len(negative_df)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# LDA PARAMETERS\n",
    "# ---------------------------\n",
    "n_topics = 3        # you can change\n",
    "top_n_words = 8\n",
    "\n",
    "# ---------------------------\n",
    "# VECTORIZE TEXT\n",
    "# ---------------------------\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "dtm = vectorizer.fit_transform(negative_df[\"Text\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# ---------------------------\n",
    "# RUN LDA\n",
    "# ---------------------------\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# ---------------------------\n",
    "# TOPIC KEYWORDS\n",
    "# ---------------------------\n",
    "topics = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_features_ind = topic.argsort()[::-1][:top_n_words]\n",
    "    top_words = [feature_names[i] for i in top_features_ind]\n",
    "    topics.append({\"topic\": f\"topic_{topic_idx}\",\n",
    "                   \"keywords\": \" \".join(top_words)})\n",
    "\n",
    "topics_df = pd.DataFrame(topics)\n",
    "print(\"\\nTop Topics & Keywords:\")\n",
    "print(topics_df)\n",
    "\n",
    "# ---------------------------\n",
    "# ASSIGN TOPIC LABEL TO EACH POST\n",
    "# ---------------------------\n",
    "doc_topic_dist = lda.transform(dtm)\n",
    "doc_topics = doc_topic_dist.argmax(axis=1)\n",
    "negative_df[\"topic_label\"] = [\"topic_\" + str(t) for t in doc_topics]\n",
    "\n",
    "# Optionally save to CSV for downstream use\n",
    "out_dir = r\"E:/internship project/data/processed\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "neg_path = os.path.join(out_dir, 'LDA2.csv')\n",
    "negative_df.to_csv(neg_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nSaved negative_df to: {neg_path}\")\n",
    "\n",
    "print(\"\\nNegative Posts with Topic Labels:\")\n",
    "print(negative_df)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 17 Completed')\n",
    "print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90947a88-2ca2-4e81-9d00-e36072576569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 18: SUMMARIZE TOPIC CLUSTERS\n",
      "================================================================================\n",
      "\n",
      "Topic Cluster Summary\n",
      "                             cluster_name  \\\n",
      "0         Poor Experience / Quality Issue   \n",
      "1    Expectation Failure / Disappointment   \n",
      "2  Regret / Not Recommend / Waste of Time   \n",
      "\n",
      "                                           key_terms  total_posts  \n",
      "0  experience, poor, overall, improved, definitel...            3  \n",
      "1  outcome, disappointed, totally, meet, expectat...            3  \n",
      "2  choosing, regret, waste, time, product, expect...            4  \n",
      "\n",
      "Saved topic summary to: E:/internship project/data/processed\\negative_topics_keywords.csv\n",
      "\n",
      "================================================================================\n",
      "✓ Task 18 Completed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 18: SUMMARIZE TOPIC CLUSTERS\n",
    "# ============================\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 18: SUMMARIZE TOPIC CLUSTERS')\n",
    "print('=' * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD DATA\n",
    "# ---------------------------\n",
    "out_dir = r\"E:/internship project/data/processed\"\n",
    "lda_file = os.path.join(out_dir, 'LDA2.csv')\n",
    "\n",
    "if 'negative_df' not in globals():\n",
    "    if os.path.exists(lda_file):\n",
    "        negative_df = pd.read_csv(lda_file)\n",
    "        print(\"Loaded LDA2.csv from disk...\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"LDA2.csv not found. Run Task 17 first!\")\n",
    "\n",
    "# ---------------------------\n",
    "# SAFETY CHECK FOR VARIABLES\n",
    "# (handles cases when Task 18 runs separately)\n",
    "# ---------------------------\n",
    "try:\n",
    "    n_topics = n_topics\n",
    "except:\n",
    "    n_topics = lda.n_components\n",
    "\n",
    "try:\n",
    "    top_n_words = top_n_words\n",
    "except:\n",
    "    top_n_words = 8\n",
    "\n",
    "# ---------------------------\n",
    "# CREATE TOPIC SUMMARY\n",
    "# ---------------------------\n",
    "summary_list = []\n",
    "\n",
    "for topic_idx in range(n_topics):\n",
    "    topic_name = f\"topic_{topic_idx}\"\n",
    "\n",
    "    # Extract top words of this topic\n",
    "    topic_words_idx = lda.components_[topic_idx].argsort()[::-1][:top_n_words]\n",
    "    topic_words = [feature_names[i] for i in topic_words_idx]\n",
    "\n",
    "    # Count how many posts belong to this topic\n",
    "    count = (negative_df[\"topic_label\"] == topic_name).sum()\n",
    "\n",
    "    summary_list.append({\n",
    "        \"cluster_name\": topic_name,\n",
    "        \"key_terms\": \", \".join(topic_words),\n",
    "        \"total_posts\": count\n",
    "    })\n",
    "\n",
    "topics_summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# ---------------------------\n",
    "# ADD HUMAN READABLE CLUSTER NAMES\n",
    "# ---------------------------\n",
    "cluster_mapping = {\n",
    "    \"topic_0\": \"Poor Experience / Quality Issue\",\n",
    "    \"topic_1\": \"Expectation Failure / Disappointment\",\n",
    "    \"topic_2\": \"Regret / Not Recommend / Waste of Time\"\n",
    "}\n",
    "\n",
    "topics_summary_df[\"cluster_name\"] = topics_summary_df[\"cluster_name\"].map(cluster_mapping)\n",
    "\n",
    "print(\"\\nTopic Cluster Summary\")\n",
    "print(topics_summary_df)\n",
    "\n",
    "# ---------------------------\n",
    "# SAVE OUTPUT\n",
    "# ---------------------------\n",
    "topics_path = os.path.join(out_dir, 'negative_topics_keywords.csv')\n",
    "topics_summary_df.to_csv(topics_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nSaved topic summary to: {topics_path}\")\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 18 Completed')\n",
    "print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c63b5ab1-2cec-4e40-9a73-4acde3f3bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 19: OVERALL TOXIC KEYWORD FREQUENCY\n",
      "================================================================================\n",
      "\n",
      "Overall Toxic Keyword Frequency:\n",
      "     toxic_word  frequency\n",
      "2           not          2\n",
      "0        regret          1\n",
      "1        failed          1\n",
      "3         waste          1\n",
      "4  disappointed          1\n",
      "5          poor          1\n",
      "6     recommend          1\n",
      "\n",
      "Saved Overall Toxic Table to: E:/internship project/data/processed\\toxic_keywords_overall.csv\n",
      "\n",
      "================================================================================\n",
      "✓ Task 19 Completed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 19: TOXIC KEYWORD FREQUENCY (OVERALL ONLY)\n",
    "# ============================\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 19: OVERALL TOXIC KEYWORD FREQUENCY')\n",
    "print('=' * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD NEGATIVE DATA\n",
    "# ---------------------------\n",
    "out_dir = r\"E:/internship project/data/processed\"\n",
    "lda_file = os.path.join(out_dir, 'LDA2.csv')\n",
    "\n",
    "if 'negative_df' not in globals():\n",
    "    if os.path.exists(lda_file):\n",
    "        negative_df = pd.read_csv(lda_file)\n",
    "        print(\"Loaded LDA2.csv from disk...\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Run Task 17 first!\")\n",
    "\n",
    "# ---------------------------\n",
    "# TOXIC LEXICON\n",
    "# (you may expand if needed)\n",
    "# ---------------------------\n",
    "toxicity_words = [\n",
    "    \"poor\",\"bad\",\"worst\",\"terrible\",\"waste\",\"useless\",\n",
    "    \"regret\",\"hate\",\"angry\",\"disappointed\",\"failed\",\n",
    "    \"not\",\"recommend\"\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# CLEAN TEXT\n",
    "# ---------------------------\n",
    "negative_df[\"clean_text\"] = negative_df[\"Text\"].str.lower()\n",
    "\n",
    "# ---------------------------\n",
    "# COUNT TOXIC WORDS (GLOBAL)\n",
    "# ---------------------------\n",
    "toxic_counter = Counter()\n",
    "\n",
    "for text in negative_df[\"clean_text\"]:\n",
    "    for word in toxicity_words:\n",
    "        if word in text:\n",
    "            toxic_counter[word] += 1\n",
    "\n",
    "toxic_df = pd.DataFrame(toxic_counter.items(),\n",
    "                        columns=[\"toxic_word\",\"frequency\"]\n",
    "                        ).sort_values(by=\"frequency\",\n",
    "                                      ascending=False)\n",
    "\n",
    "print(\"\\nOverall Toxic Keyword Frequency:\")\n",
    "print(toxic_df)\n",
    "\n",
    "# ---------------------------\n",
    "# SAVE OUTPUT\n",
    "# ---------------------------\n",
    "save_path = os.path.join(out_dir, \"toxic_keywords_overall.csv\")\n",
    "toxic_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved Overall Toxic Table to: {save_path}\")\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 19 Completed')\n",
    "print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7b20e91-7121-4595-b44f-5f37f220244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 20: EXPORT ALL ROOT-CAUSE TABLES\n",
      "================================================================================\n",
      "Exported: E:/internship project/data/processed\\topic_clusters.csv\n",
      "Exported: E:/internship project/data/processed\\negative_keywords.csv\n",
      "Exported: E:/internship project/data/processed\\toxic_keywords.csv\n",
      "\n",
      "================================================================================\n",
      "✓ Task 20 Completed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 20: EXPORT ROOT-CAUSE TABLES\n",
    "# ============================\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TASK 20: EXPORT ALL ROOT-CAUSE TABLES')\n",
    "print('=' * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "out_dir = r\"E:/internship project/data/processed\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 1️⃣ Topic Clusters Table (From Task 18)\n",
    "# ---------------------------\n",
    "topic_file = os.path.join(out_dir, \"negative_topics_keywords.csv\")\n",
    "topic_clusters_path = os.path.join(out_dir, \"topic_clusters.csv\")\n",
    "\n",
    "if os.path.exists(topic_file):\n",
    "    topic_df = pd.read_csv(topic_file)\n",
    "    topic_df.to_csv(topic_clusters_path, index=False)\n",
    "    print(f\"Exported: {topic_clusters_path}\")\n",
    "else:\n",
    "    print(\"Task 18 result not found! Run Task 18 first.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2️⃣ Negative Keywords Table\n",
    "# (We will reuse same topic file keywords)\n",
    "# ---------------------------\n",
    "negative_keywords_path = os.path.join(out_dir, \"negative_keywords.csv\")\n",
    "\n",
    "if os.path.exists(topic_file):\n",
    "    negative_keywords_df = topic_df[[\"cluster_name\",\"key_terms\"]]\n",
    "    negative_keywords_df.to_csv(negative_keywords_path, index=False)\n",
    "    print(f\"Exported: {negative_keywords_path}\")\n",
    "else:\n",
    "    print(\"Negative keywords skipped because Task 18 file missing.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3️⃣ Toxic Keywords Table (From Task 19)\n",
    "# ---------------------------\n",
    "toxic_file = os.path.join(out_dir, \"toxic_keywords_overall.csv\")\n",
    "toxic_export_path = os.path.join(out_dir, \"toxic_keywords.csv\")\n",
    "\n",
    "if os.path.exists(toxic_file):\n",
    "    toxic_df = pd.read_csv(toxic_file)\n",
    "    toxic_df.to_csv(toxic_export_path, index=False)\n",
    "    print(f\"Exported: {toxic_export_path}\")\n",
    "else:\n",
    "    print(\"Task 19 toxic output not found! Run Task 19 first.\")\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✓ Task 20 Completed')\n",
    "print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089c195-69ac-439f-a957-2166d18058aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
